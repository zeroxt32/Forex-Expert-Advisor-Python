{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc2ea3a-3fd7-42f8-9377-894500b808d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random, shutil, pickle, sys\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from keras import Model\n",
    "import keras, os, glob\n",
    "import tensorflow as tf\n",
    "from keras.layers import Layer, Dense, Conv2D, Flatten, RepeatVector,Dropout, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
    "from keras.layers import Activation, LSTM, Bidirectional , Dropout\n",
    "from keras import layers\n",
    "# import cv2\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "import codecs\n",
    "import csv\n",
    "import secrets\n",
    "import sqlite3\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "import datetime\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import math, datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f0d813-d93a-4a54-9179-4d713d31d0c6",
   "metadata": {},
   "source": [
    "Chart Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4875963e-ec38-44c8-9a67-e9f9ace56b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChartDecorator:\n",
    "    def __init__(self):\n",
    "        self.balance_limit = 300\n",
    "        self.standard_balance = 1000\n",
    "        self.prev_trade = None\n",
    "        \n",
    "    def add_top_bottom_bar(self, img, draw_context):\n",
    "        # Define the coor dinates of the black bar\n",
    "        x1, y1 = 0, 204\n",
    "        x2, y2 = 200, 224\n",
    "        # Draw the black bar bottom bar\n",
    "        draw_context.rectangle([x1, y1, x2, y2], fill=(0, 0, 0))\n",
    "        # Draw the black bar top bar\n",
    "        x1, y1 = 0, 0\n",
    "        x2, y2 = 224, 14\n",
    "        draw_context.rectangle([x1, y1, x2, y2], fill=(0, 0, 0))\n",
    "\n",
    "\n",
    "    def draw_account_balance(self, img, draw_context, account_balance = 1000):\n",
    "        balance = self.balance_limit + (account_balance - self.standard_balance)\n",
    "        if balance == self.balance_limit:\n",
    "            x1, y1 = 3, 204\n",
    "            x2, y2 = 75 , 224\n",
    "            # Draw the green bar for account\n",
    "            draw_context.rectangle([x1, y1, x2, y2], fill=(110, 235, 131))\n",
    "\n",
    "        elif balance < self.balance_limit:\n",
    "            health_bar = int((balance * 75) / self.balance_limit )\n",
    "            lost_bar = 75 - health_bar\n",
    "            #issue 1\n",
    "            # print(f\"issue 1 balance {balance}\")\n",
    "            # print(f\"health bar {health_bar}\")\n",
    "            \n",
    "            # print(f\"lost bar {lost_bar}\")\n",
    "            #current_balnace\n",
    "            x1, y1 = 3, 204\n",
    "            x2, y2 = health_bar, 224\n",
    "            draw_context.rectangle([x1, y1, x2, y2], fill=(110, 235, 131))\n",
    "            #lost bar\n",
    "            x1, y1 = health_bar, 204\n",
    "            x2, y2 = 75 , 224\n",
    "            draw_context.rectangle([x1, y1, x2, y2], fill=(255, 0, 0))\n",
    "        elif balance > self.balance_limit:\n",
    "            x1, y1 = 3, 204\n",
    "            x2, y2 = 75 , 224\n",
    "            # Draw the green bar for account\n",
    "            draw_context.rectangle([x1, y1, x2, y2], fill=(110, 235, 131)) \n",
    "            #Draw the profit section on account_bar\n",
    "            profit = int(((balance - self.balance_limit) * 75) / self.balance_limit)\n",
    "\n",
    "            x1, y1 = 80, 204\n",
    "            x2, y2 = 80 + profit , 224\n",
    "            # Draw the green bar for account\n",
    "            draw_context.rectangle([x1, y1, x2, y2], fill=(0,0,255))# fill=(71, 44, 27)) \n",
    "\n",
    "        #draw start separator\n",
    "        #draw Account Separator and end separator\n",
    "        x1, y1 = 0, 204\n",
    "        x2, y2 = 3 , 224\n",
    "        # Draw the green bar for account\n",
    "        draw_context.rectangle([x1, y1, x2, y2], fill=(255, 87, 20))     \n",
    "\n",
    "        #draw Account Separator and end separator\n",
    "        x1, y1 = 76, 204\n",
    "        x2, y2 = 80 , 224\n",
    "        # Draw the green bar for account\n",
    "        draw_context.rectangle([x1, y1, x2, y2], fill=(255, 87, 20)) \n",
    "\n",
    "        #draw Account Separator and end separator\n",
    "        x1, y1 = 155, 204\n",
    "        x2, y2 = 160 , 224\n",
    "        # Draw the green bar for account\n",
    "        draw_context.rectangle([x1, y1, x2, y2], fill=(255, 87, 20)) \n",
    "\n",
    "    def draw_current_trade(self,img, draw_context, position=None):\n",
    "        if position == \"buy\":\n",
    "            x1, y1 = 160, 204\n",
    "            x2, y2 = 175, 224\n",
    "            draw_context.rectangle([x1, y1, x2, y2], fill=(255, 255, 0)) \n",
    "        elif position == \"sell\":\n",
    "            x1, y1 = 180, 204\n",
    "            x2, y2 = 195, 224\n",
    "            draw_context.rectangle([x1, y1, x2, y2], fill=(0, 255, 0))  \n",
    "        else:\n",
    "            x1, y1 = 200, 204\n",
    "            x2, y2 = 215, 224\n",
    "            draw_context.rectangle([x1, y1, x2, y2], fill=(255,0,255)) \n",
    "\n",
    "        #Draw position separator icons\n",
    "        x1, y1 = 175, 204\n",
    "        x2, y2 = 180, 224\n",
    "        draw_context.rectangle([x1, y1, x2, y2], fill=(255, 255, 255)) \n",
    "        x1, y1 = 195, 204\n",
    "        x2, y2 = 200, 224\n",
    "        draw_context.rectangle([x1, y1, x2, y2], fill=(255, 255, 255)) \n",
    "        x1, y1 = 215, 204\n",
    "        x2, y2 = 220, 224\n",
    "        draw_context.rectangle([x1, y1, x2, y2], fill=(255, 255, 255)) \n",
    "\n",
    "    def draw_profit_bar(self, img, draw_context, profit = 0, account_balance = 1000, position = None):\n",
    "        pnl_parts = int(abs(profit) / 10)\n",
    "        for segment in range(0, 200, 10):\n",
    "            f = segment + 10\n",
    "            x1, y1 = segment+10, 0\n",
    "            x2, y2 = f + 1, 9\n",
    "            draw_context.rectangle([x1, y1, x2, y2], fill=(255, 255, 255)) \n",
    "            #display profit on the chart\n",
    "            if profit > 0 and pnl_parts > 0 and segment >= 100:\n",
    "                if segment < pnl_parts * 10 + 100:\n",
    "                    x1, y1 = segment, 0\n",
    "                    x2, y2 = segment + 10, 9\n",
    "                    draw_context.rectangle([x1, y1, x2, y2], fill=(27, 152, 224)) \n",
    "            elif profit < 0 and pnl_parts > 0 and segment <= 90 and segment >= 100 - abs(profit):           \n",
    "                x1, y1 = segment, 0\n",
    "                x2, y2 = segment + 10, 9\n",
    "                draw_context.rectangle([x1, y1, x2, y2], fill=(255, 0, 0))  \n",
    "            #draw progressive account balance below profit bar\n",
    "            account_balance_bar = (200*account_balance)/2000\n",
    "            x1, y1 = 0, 10\n",
    "            x2, y2 = account_balance_bar, 14\n",
    "            draw_context.rectangle([x1, y1, x2, y2], fill=(255, 119, 0)) \n",
    "            #Add a closing indication at the top of the \n",
    "            if position == \"close\" and profit > 0 and self.prev_trade[2] == 1:\n",
    "                x1, y1 = 200, 0\n",
    "                x2, y2 = 224, 14\n",
    "                draw_context.rectangle([x1, y1, x2, y2], fill=(255, 255, 0))\n",
    "            elif position == \"close\" and profit > 0 and self.prev_trade == 2:\n",
    "                x1, y1 = 200, 0\n",
    "                x2, y2 = 224, 14\n",
    "                draw_context.rectangle([x1, y1, x2, y2], fill=(0, 255, 0))\n",
    "            elif position == \"close\" and profit < 0 and self.prev_trade in [2,1]:\n",
    "                x1, y1 = 200, 0\n",
    "                x2, y2 = 224, 14\n",
    "                draw_context.rectangle([x1, y1, x2, y2], fill=(255,0, 0))\n",
    "       \n",
    "    def draw_buy_bar(self, img2, start_lines, end_lines, position = None, profit = 0, prev_trade=None, account_balance = 1100 ):\n",
    "        self.prev_trade = prev_trade\n",
    "        \n",
    "        from PIL import Image, ImageDraw\n",
    "        d = ImageDraw.Draw(img2)\n",
    "        \n",
    "        self.add_top_bottom_bar(img2, d)\n",
    "        self.draw_account_balance(img2, d, account_balance)\n",
    "\n",
    "        if position == \"sell\" and 2 in [self.prev_trade[3]]:\n",
    "            #draw icon for trade setup\n",
    "            self.draw_current_trade(img2, d, \"sell\")\n",
    "            fixed_ratio = False\n",
    "            #issue fix bug \n",
    "            #a situation where there is profit but since the trade was entered there were pertuabations of zooming and changing of window prices indicated on the subwindow\n",
    "            #fix this by giving a ratio use profit to check if we are profitable, then check if end lines are lower than start lines , use a ratio of 1px for 3 points\n",
    "            #when scaling the bar check the distance remaining behind you at the top of the window. start from index 14 your calculations\n",
    "            if profit >= 0 and start_lines[\"ask_line\"] > end_lines[\"ask_line\"] :\n",
    "                # print(f\"\\nRatio bug found {end_lines}\\n\")\n",
    "                ratio = 15 #int((start_lines[\"ask_line\"] - end_lines[\"ask_line\"] ) / 3)\n",
    "                if end_lines[\"ask_line\"] - ratio < 14:\n",
    "                    start_lines[\"ask_line\"] = 14\n",
    "                    start_lines[\"bid_line\"] = 18\n",
    "                else:\n",
    "                    start_lines[\"ask_line\"] = end_lines[\"ask_line\"] - ratio\n",
    "                    start_lines[\"bid_line\"] = end_lines[\"bid_line\"] - ratio\n",
    "                fixed_ratio = True\n",
    "\n",
    "            # Draw indicator of trade start\n",
    "            \n",
    "            line_color = (200,200,200)\n",
    "            top = (200, end_lines[\"ask_line\"])\n",
    "            bottom = (200, end_lines[\"bid_line\"])\n",
    "            d.line([top, bottom], fill=line_color, width=20)\n",
    "\n",
    "            #Draw start marker for the trade\n",
    "            line_color = (0, 255, 0)\n",
    "            top = (200, start_lines[\"ask_line\"])\n",
    "            bottom = (200, start_lines[\"bid_line\"])\n",
    "            d.line([top, bottom], fill=line_color, width=30) \n",
    "            \n",
    "            #Draw start line for trade entry\n",
    "            line_color = (0, 255, 0)#(252, 81, 48)\n",
    "            top = (50, start_lines[\"ask_line\"])\n",
    "            bottom = (50, start_lines[\"bid_line\"])\n",
    "            d.line([top, bottom], fill=line_color, width=250) \n",
    "\n",
    "            #Draw profit indicator of sell\n",
    "            if profit < 0:\n",
    "                #Draw the main bar of the trade position\n",
    "                if fixed_ratio:\n",
    "                    line_color = (255, int(ratio/2+10), 0)\n",
    "                    top = (200, start_lines[\"ask_line\"])\n",
    "                    bottom = (200,end_lines[\"bid_line\"])\n",
    "                    d.line([top, bottom], fill=line_color, width=int(ration/5)+10)\n",
    "                    fixed_ratio = False\n",
    "                else:\n",
    "                    line_color = (255, 0, 0)\n",
    "                    top = (200, start_lines[\"ask_line\"])\n",
    "                    bottom = (200,end_lines[\"bid_line\"])\n",
    "                    d.line([top, bottom], fill=line_color, width=10)\n",
    "                \n",
    "            else:\n",
    "                line_color = (0, 255, 0)\n",
    "                top = (200, start_lines[\"ask_line\"])\n",
    "                bottom = (200,end_lines[\"bid_line\"])\n",
    "                d.line([top, bottom], fill=line_color, width=10)\n",
    "            #Draw loss indicator for sell\n",
    "            self.draw_profit_bar(img2, d, profit)\n",
    "        elif position == \"buy\" and 1 in [self.prev_trade[3]]:\n",
    "            #issue fix bug 2\n",
    "            if profit >= 0 and end_lines[\"ask_line\"] > start_lines[\"ask_line\"]:\n",
    "                ratio = 15\n",
    "                if end_lines[\"bid_line\"] + ratio > 204:\n",
    "                    start_lines[\"ask_line\"] = 200\n",
    "                    start_lines[\"bid_line\"] = 204\n",
    "                else:\n",
    "                    start_lines[\"ask_line\"] = end_lines[\"ask_line\"] + ratio\n",
    "                    start_lines[\"bid_line\"] = end_lines[\"bid_line\"] + ratio         \n",
    "            \n",
    "            #draw bottom right trade status sell , buy , close\n",
    "            self.draw_current_trade(img2, d, \"buy\") # \n",
    "            #Draw indicator of trade start\n",
    "            #Draw end marker\n",
    "            line_color = (65, 64, 102)\n",
    "            top = (200, end_lines[\"ask_line\"])\n",
    "            bottom = (200, end_lines[\"bid_line\"])\n",
    "            d.line([top, bottom], fill=line_color, width=20) \n",
    "\n",
    "            #Draw start marker for buy\n",
    "            line_color = (255, 255, 0)\n",
    "            top = (200, start_lines[\"ask_line\"])\n",
    "            bottom = (200, start_lines[\"bid_line\"])\n",
    "            d.line([top, bottom], fill=line_color, width=30) \n",
    "\n",
    "            #Draw start buy line across the chart\n",
    "            line_color = (255, 255, 0)#(255,0,200)\n",
    "            top = (50, start_lines[\"ask_line\"])\n",
    "            bottom = (50, start_lines[\"bid_line\"])\n",
    "            d.line([top, bottom], fill=line_color, width=250)         \n",
    "\n",
    "\n",
    "            if profit < 0:\n",
    "                line_color = (255, 0, 0)\n",
    "                top = (200, start_lines[\"ask_line\"])\n",
    "                bottom = (200, end_lines[\"bid_line\"])\n",
    "                d.line([top, bottom], fill=line_color, width=10)\n",
    "            else:\n",
    "                line_color = (255, 255, 0)\n",
    "                top = (200, start_lines[\"ask_line\"])\n",
    "                bottom = (200, end_lines[\"bid_line\"])\n",
    "                d.line([top, bottom], fill=line_color, width=10)\n",
    "\n",
    "            self.draw_profit_bar(img2, d, profit)\n",
    "\n",
    "\n",
    "        elif position in [\"buy\", \"sell\"] and self.prev_trade[3] == 0:\n",
    "            # print(f\"Draw buy bar \\n\\nElse{position}\\n\\n:\")\n",
    "            #draw main vertical bar\n",
    "            self.draw_current_trade(img2, d, \"close\")\n",
    "\n",
    "            line_color = (255,0,255)\n",
    "            top = (200, start_lines[\"ask_line\"])\n",
    "            bottom = (200, end_lines[\"bid_line\"])\n",
    "            d.line([top, bottom], fill=line_color, width=15)\n",
    "            #draw end bar\n",
    "            line_color = (255,0,255)\n",
    "            top = (200, end_lines[\"ask_line\"])\n",
    "            bottom = (200, end_lines[\"bid_line\"])\n",
    "            d.line([top, bottom], fill=line_color, width=30) \n",
    "            #Draw start marker and line\n",
    "            line_color = (255,0,255)\n",
    "            top = (200, start_lines[\"ask_line\"])\n",
    "            bottom = (200, start_lines[\"bid_line\"])\n",
    "            d.line([top, bottom], fill=line_color, width=30)\n",
    "\n",
    "            line_color = (2, 8, 135)\n",
    "            top = (50, start_lines[\"ask_line\"])\n",
    "            bottom = (50, start_lines[\"bid_line\"])\n",
    "            d.line([top, bottom], fill=line_color, width=250)\n",
    "\n",
    "            # self.draw_profit_bar(img2, d, profit)\n",
    "\n",
    "            if self.prev_trade[2] == 2 and profit > 0:   \n",
    "                #print right bar for the trade that has closed\n",
    "                line_color = (0, 255, 0)\n",
    "                top = (213, start_lines[\"ask_line\"])\n",
    "                bottom = (213, end_lines[\"bid_line\"])\n",
    "                d.line([top, bottom], fill=line_color, width=10) \n",
    "\n",
    "                line_color = (241, 254, 198)\n",
    "                top = (190, start_lines[\"ask_line\"])\n",
    "                bottom = (190, end_lines[\"bid_line\"])\n",
    "                d.line([top, bottom], fill=line_color, width=10)             \n",
    "\n",
    "            elif self.prev_trade[2] == 2 and profit < 0:\n",
    "                line_color = (0, 255, 0)\n",
    "                top = (213, start_lines[\"ask_line\"])\n",
    "                bottom = (213, end_lines[\"bid_line\"])\n",
    "                d.line([top, bottom], fill=line_color, width=10) \n",
    "\n",
    "                line_color = (255,0,0)\n",
    "                top = (190, start_lines[\"ask_line\"])\n",
    "                bottom = (190, end_lines[\"bid_line\"])\n",
    "                d.line([top, bottom], fill=line_color, width=10)    \n",
    "            elif self.prev_trade[2] == 1 and profit > 0:\n",
    "                # print(f\"\\n\\nHey I was here\\n\\n{self.prev_trade}\")\n",
    "                \n",
    "                line_color = (255, 255, 0)\n",
    "                top = (213, start_lines[\"ask_line\"])\n",
    "                bottom = (213, end_lines[\"bid_line\"])\n",
    "                d.line([top, bottom], fill=line_color, width=10) \n",
    "\n",
    "                line_color = (241, 254, 198)\n",
    "                top = (190, start_lines[\"ask_line\"])\n",
    "                bottom = (190, end_lines[\"bid_line\"])\n",
    "                d.line([top, bottom], fill=line_color, width=10)    \n",
    "\n",
    "            elif self.prev_trade[2] == 1 and profit < 0:\n",
    "                line_color = (255, 255, 0)\n",
    "                top = (213, start_lines[\"ask_line\"])\n",
    "                bottom = (213, end_lines[\"bid_line\"])\n",
    "                d.line([top, bottom], fill=line_color, width=10) \n",
    "\n",
    "                line_color = (255,0,0)\n",
    "                top = (190, start_lines[\"ask_line\"])\n",
    "                bottom = (190, end_lines[\"bid_line\"])\n",
    "                d.line([top, bottom], fill=line_color, width=10) \n",
    "        else :#:position == \"close\" and self.prev_trade[2] == 0:\n",
    "            self.draw_current_trade(img2, d, \"close\")\n",
    "            # print(\"I run for position Close\")\n",
    "            line_color = (255,0,255)\n",
    "            top = (200, end_lines[\"ask_line\"])\n",
    "            bottom = (200, end_lines[\"bid_line\"])\n",
    "            d.line([top, bottom], fill=line_color, width=30)  \n",
    "\n",
    "        self.draw_profit_bar(img2, d, profit, account_balance, position)\n",
    "        return img2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad49a87-8e21-4fb5-865e-c372edbf15f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e3faff-91ff-49cb-946d-8c3720dc8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForexCustomEnv:\n",
    "    def __init__(self, m1short, m5short, m1long, m5long, chart_decorator):\n",
    "        # super(BaseEnv, self).__init__()\n",
    "        self.m1short = m1short\n",
    "        self.m5short = m5short\n",
    "        self.m1long = m1long\n",
    "        self.m5long = m5long\n",
    "        self.trade_queue_m5_short = deque(maxlen=4)\n",
    "        self.image_queue_m5_short = deque(maxlen=4)\n",
    "        self.image_queue_m5_long = deque(maxlen=4)\n",
    "        self.trade_queue_m5_long = deque(maxlen=4)\n",
    "        self.dataset_directory = \"M1M5Charts/\"#\"colabM1M5/episode2/\"\n",
    "        self.account_balance = 1000\n",
    "        self.env_draw = False\n",
    "        self.chart_decorator = chart_decorator\n",
    "        \n",
    "        self.prev_trade = deque(maxlen=4)\n",
    "        self.prev_trade.append(0)\n",
    "        self.prev_trade.append(0)\n",
    "        self.prev_trade.append(0)\n",
    "        self.prev_trade.append(0)\n",
    "        \n",
    "        self.current_step = 4\n",
    "        self.action = 0\n",
    "        self.position = \"close\"\n",
    "        self.reward_ma = deque(maxlen=3)\n",
    "        self.reward_Q = deque(maxlen=2)\n",
    "        self.observation_space = (1,1,224,224,3)\n",
    "        self.space = (1,1,224,224,3)\n",
    "        self.action_space = 3\n",
    "        \n",
    "        # self.scaler = MinMaxScaler(feature_range=(0, 1)).fit(np.array([0,0,20]).reshape(-1,1))\n",
    "    #base env compatibility\n",
    "    def try_reset(self):\n",
    "        return self.reset()\n",
    "    def get_ask_bid_lines(self, image_path):\n",
    "        ask_bid_lines = {\"ask_line\":None, \"bid_line\":None, \"key\": 0}\n",
    "        img = Image.open(f\"{self.dataset_directory}{image_path}\")\n",
    "        img_data = np.array(img)\n",
    "        last_part = Image.fromarray(img_data[:,200:201,:])\n",
    "        img1_pixels = last_part.load()\n",
    "        ask_color = (255,0,0)\n",
    "        bid_color = (119, 136, 153)\n",
    "        for y in range(224):\n",
    "            if img1_pixels[0,y] == ask_color:\n",
    "                ask_bid_lines[\"ask_line\"] = y\n",
    "            if img1_pixels[0,y] == bid_color:\n",
    "                ask_bid_lines[\"bid_line\"] = y\n",
    "        if ask_bid_lines[\"ask_line\"] == None:\n",
    "            # print(\"Gotcha\")\n",
    "            ask_bid_lines[\"ask_line\"] = ask_bid_lines[\"bid_line\"] + 3\n",
    "            ask_bid_lines[\"bid_line\"] = ask_bid_lines[\"bid_line\"] + 6\n",
    "            \n",
    "        return ask_bid_lines , img        \n",
    "    \n",
    "    def reset(self, current_step = 4, step = 50):\n",
    "        self.current_step = current_step\n",
    "        self.step_size = step\n",
    "        # print(f\"DBG reset {self.m1short.iloc[self.current_step - 4]['image_path']}\")\n",
    "\n",
    "        linesq_m5_short, imageq_m5_short = self.get_ask_bid_lines(f\"{self.m5short.iloc[self.current_step - 4]['image_path']}\")\n",
    "        linesq_m5_short[\"key\"] = current_step - 4\n",
    "        linest_m5_short, imaget_m5_short = self.get_ask_bid_lines(f\"{self.m5short.iloc[self.current_step - 3]['image_path']}\")\n",
    "        linest_m5_short[\"key\"] = current_step - 3\n",
    "        liness_m5_short, images_m5_short = self.get_ask_bid_lines(f\"{self.m5short.iloc[self.current_step - 2]['image_path']}\")\n",
    "        liness_m5_short[\"key\"] = current_step - 2\n",
    "        linesc_m5_short, imagec_m5_short = self.get_ask_bid_lines(f\"{self.m5short.iloc[self.current_step - 1]['image_path']}\")\n",
    "        linesc_m5_short[\"key\"] = current_step - 1\n",
    "        \n",
    "        linesq_m5_long, imageq_m5_long = self.get_ask_bid_lines(f\"{self.m5long.iloc[self.current_step - 4]['image_path']}\")\n",
    "        linesq_m5_long[\"key\"] = current_step - 4\n",
    "        linest_m5_long, imaget_m5_long = self.get_ask_bid_lines(f\"{self.m5long.iloc[self.current_step - 3]['image_path']}\")\n",
    "        linest_m5_long[\"key\"] = current_step - 3\n",
    "        liness_m5_long, images_m5_long = self.get_ask_bid_lines(f\"{self.m5long.iloc[self.current_step - 2]['image_path']}\")\n",
    "        liness_m5_long[\"key\"] = current_step - 2\n",
    "        linesc_m5_long, imagec_m5_long = self.get_ask_bid_lines(f\"{self.m5long.iloc[self.current_step - 1]['image_path']}\")\n",
    "        linesc_m5_long[\"key\"] = current_step - 1\n",
    "\n",
    "        self.trade_queue_m5_short.append(linesq_m5_short)\n",
    "        self.trade_queue_m5_short.append(linest_m5_short)\n",
    "        self.trade_queue_m5_short.append(liness_m5_short)\n",
    "        self.trade_queue_m5_short.append(linesc_m5_short)\n",
    "        self.image_queue_m5_short.append(imageq_m5_short)\n",
    "        self.image_queue_m5_short.append(imaget_m5_short)\n",
    "        self.image_queue_m5_short.append(images_m5_short)\n",
    "        self.image_queue_m5_short.append(imagec_m5_short)         \n",
    "        \n",
    "        self.trade_queue_m5_long.append(linesq_m5_long)\n",
    "        self.trade_queue_m5_long.append(linest_m5_long)\n",
    "        self.trade_queue_m5_long.append(liness_m5_long)\n",
    "        self.trade_queue_m5_long.append(linesc_m5_long)\n",
    "        self.image_queue_m5_long.append(imageq_m5_long)\n",
    "        self.image_queue_m5_long.append(imaget_m5_long)\n",
    "        self.image_queue_m5_long.append(images_m5_long)\n",
    "        self.image_queue_m5_long.append(imagec_m5_long)        \n",
    "        \n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        self.position = \"close\"\n",
    "        self.reset_current_trade()\n",
    "        \n",
    "        next_state = self.get_obs()\n",
    "        self.episode_profit = 0\n",
    "        self.prev_trade = deque(maxlen=4)\n",
    "        self.prev_trade.append(0)\n",
    "        self.prev_trade.append(0)\n",
    "        self.prev_trade.append(0)\n",
    "        self.prev_trade.append(0)\n",
    "        self.action = 0\n",
    "        self.reward_ma = deque(maxlen=3)\n",
    "        self.reward_Q = deque(maxlen=3)\n",
    "\n",
    "        #reset standard limit in chart decorator\n",
    "        self.chart_decorator.standard_limit = self.account_balance\n",
    "        \n",
    "        return next_state, self.reward, self.done, self.current_trade \n",
    "        \n",
    "    def reset_current_trade(self ):\n",
    "        self.current_trade = {}\n",
    "        self.current_trade[\"current_step\"] = self.current_step\n",
    "        self.current_trade[\"ask_bid_lines_m5_short\"] = None\n",
    "        self.current_trade[\"ask_bid_lines_m5_long\"] = None\n",
    "        self.current_trade[\"entry_price\"] = None\n",
    "        self.current_trade[\"current_price\"] = None\n",
    "        self.current_trade[\"timesteps\"] = 0\n",
    "        self.current_trade[\"position\"] = \"close\"\n",
    "        self.current_trade[\"profit\"] = 0\n",
    "        self.current_trade[\"parent_ask_bid_lines_m5_short\"] = None\n",
    "        self.current_trade[\"parent_ask_bid_lines_m5_long\"] = None\n",
    "        self.current_trade[\"reward\"] = 0\n",
    "        self.current_trade[\"balance\"] = self.account_balance\n",
    "        self.current_trade[\"episode_profit\"] = 0\n",
    "        #this code is to add dimensions to 8 of them 4 for m1short 4 for m5short remaining to draw on m5 charts\n",
    "        self.current_trade[\"ask_bid_lines_m5_short\"] = None\n",
    "        \n",
    "        self.current_trade[\"highest\"] = 0\n",
    "        self.current_trade[\"price_ma\"] = deque(maxlen=3)\n",
    "        self.current_trade[\"reward_ma\"] = deque(maxlen=3)\n",
    "        self.account_balance = 1000\n",
    "        self.old_balance = 1000\n",
    "        \n",
    "        \n",
    "    def display_stacked_horizontally(self,imageq, imaget,images,imagec):\n",
    "        # # Calculate the required dimensions for the new image\n",
    "        new_width = imageq.width * 4\n",
    "        new_height = imageq.height\n",
    "        # # Create a new image with the required dimensions\n",
    "        new_image = Image.new(\"RGB\", (new_width, new_height))\n",
    "        # # Paste the individual images side by side\n",
    "        new_image.paste(imageq, (0, 0))\n",
    "        new_image.paste(imaget, (imageq.width, 0))\n",
    "        new_image.paste(images, (imageq.width + imaget.width, 0))\n",
    "        new_image.paste(imagec, (imageq.width + imaget.width + images.width, 0))\n",
    "        \n",
    "\n",
    "    \n",
    "    def reset_collection(self, t_q, i_q):\n",
    "        _queue = deque(maxlen=4)\n",
    "        for i in range(0, 4):\n",
    "            _lines_c, _image_c = self.get_ask_bid_lines(f\"{self.dataset_directory}{self.m1short.iloc[t_q[i]['key']]['image_path']}\")\n",
    "            _image_c = self.draw_buy_bar(_image_c, _lines_c, _lines_c, position=\"close\", profit = 0)\n",
    "            _queue.append(_image_c)\n",
    "        return t_q, _queue\n",
    "    \n",
    "    def draw_collection(self,q, q2):\n",
    "        # # Calculate the required dimensions for the new image\n",
    "        new_width = q[0].width * 4\n",
    "        new_height =q[0].height\n",
    "        # # Create a new image with the required dimensions\n",
    "        new_image = Image.new(\"RGB\", (new_width, new_height))\n",
    "        # # Paste the individual images side by side\n",
    "        # new_image.paste(q[0], (0, 0))\n",
    "        # new_image.paste(q2[1], (0, 0))\n",
    "        # new_image.paste(q2[2], (q[0].width, 0))\n",
    "        new_image.paste(q2[3], (q[0].width + q[0].width, 0))\n",
    "        # new_image.paste(q[3], (q[0].width + q[0].width + q[0].width, 0))\n",
    "        display(q2[3])\n",
    "        \n",
    "    def draw_buy_bar(self, current_image, start_lines, end_lines, position = None, profit = 0, prev_trade=None):\n",
    "        # self.position = position\n",
    "        # self.profit = profit\n",
    "        return self.chart_decorator.draw_buy_bar(current_image, start_lines, end_lines, self.position, profit, prev_trade, self.account_balance)\n",
    "\n",
    "    \n",
    "    '''\n",
    "    @description: Enters a trade in the forex environment\n",
    "                  Enters the trade in the current time step, sets the position of the trade. sets the ask bid lines for the current frame\n",
    "                  sets the entry price for the trade, and profit = 0. Then draws on the image the entry icon on the chart\n",
    "                  and saves the updated current_image with modifications to the image queue with its corresponding ask and bid lines. \n",
    "                  then initializes the timesteps of the current trade to 0. sets the reward to 0.2\n",
    "    @params\n",
    "                  @position = None the position of the current_trade\n",
    "                  @ask_bid_lines of the current_image\n",
    "                  @current_image the current frame of the environment\n",
    "                  \n",
    "    '''\n",
    "    \n",
    "\n",
    "\n",
    "    # def softplus_reward(self, profit, timestep, highest=0,position = \"close\", threshold=0.8):\n",
    "\n",
    "    #     loss = min(profit,0)\n",
    "    #     if profit == 0: \n",
    "    #         profit = 1\n",
    "    #     if highest == 0:\n",
    "    #         highest = 1\n",
    "    #     if profit < 0:\n",
    "    #         return -np.log(abs(profit)) #,-np.log(abs(profit))/10+np.exp(-timestep/10)\n",
    "    #     return np.log(profit) #+ np.log( profit )-np.log(highest)\n",
    "\n",
    "    # def get_reward(self, c_trade):\n",
    "\n",
    "    #     return self.softplus_reward(c_trade[\"profit\"], c_trade[\"timesteps\"],  c_trade[\"highest\"], c_trade[\"position\"])\n",
    "\n",
    "    \n",
    "    \n",
    "    def enter_trade(self, ask_bid_lines_m1_short=None, ask_bid_lines_m5_short = None, ask_bid_lines_m1_long = None, ask_bid_lines_m5_long = None, current_image_m1_short=None, current_image_m5_short=None, current_image_m1_long=None, current_image_m5_long = None):\n",
    "        self.current_trade[\"position\"] = self.position\n",
    "        \n",
    "        self.current_trade[\"ask_bid_lines_m5_short\"] = ask_bid_lines_m5_short\n",
    "        self.current_trade[\"ask_bid_lines_m5_long\"] = ask_bid_lines_m5_long\n",
    "        self.current_trade[\"entry_price\"] = float(f\"{self.m5short.iloc[self.current_step]['Ask']}\") if self.position == \"buy\" else float(f\"{self.m5short.iloc[self.current_step]['Bid']}\")\n",
    "        self.current_trade[\"profit\"] = 0\n",
    "        self.current_trade[\"current_price\"] = float(f\"{self.m5short.iloc[self.current_step]['Ask']}\")\n",
    "        self.current_trade[\"timesteps\"] = 1\n",
    "        \n",
    "        self.current_trade[\"parent_ask_bid_lines_m5_short\"] = ask_bid_lines_m5_short\n",
    "        self.current_trade[\"parent_ask_bid_lines_m5_long\"] = ask_bid_lines_m5_long\n",
    "        self.current_trade[\"balance\"] = self.account_balance\n",
    "        self.current_trade[\"highest\"] = 0\n",
    "        self.current_trade[\"price_ma\"] = deque(maxlen=2)\n",
    "        self.current_trade[\"reward_ma\"] = deque(maxlen=2)\n",
    "        #self.current_trade[\"highest\"] += 5\n",
    "        \n",
    "        self.current_trade[\"price_ma\"].append(0)\n",
    "        self.current_trade[\"price_ma\"].append(0)\n",
    "        self.current_trade[\"price_ma\"].append(0)\n",
    "\n",
    "        \n",
    "        self.current_trade[\"reward_ma\"].append(0.10)\n",
    "        self.current_trade[\"reward_ma\"].append(0.10)\n",
    "        self.current_trade[\"reward_ma\"].append(0.10)\n",
    "\n",
    "        \n",
    "        current_image_m5_short = self.draw_buy_bar(current_image_m5_short, ask_bid_lines_m5_short, ask_bid_lines_m5_short, self.position , self.current_trade[\"profit\"], self.prev_trade )\n",
    "        current_image_m5_long = self.draw_buy_bar(current_image_m5_long, ask_bid_lines_m5_long, ask_bid_lines_m5_long, self.position, self.current_trade[\"profit\"], self.prev_trade)\n",
    "        self.trade_queue_m5_short.append(ask_bid_lines_m5_short)\n",
    "        self.trade_queue_m5_long.append(ask_bid_lines_m5_long)\n",
    "        self.image_queue_m5_short.append(current_image_m5_short)  \n",
    "        self.image_queue_m5_long.append(current_image_m5_long)\n",
    "        \n",
    "        if self.env_draw:\n",
    "            \n",
    "            self.draw_collection(self.image_queue_m5_short,self.image_queue_m5_long) \n",
    "            # self.draw_collection(self.image_queue_m5_short,self.image_queue_m5_long) \n",
    "        self.reward = 0# self.get_reward(self.current_trade) #self.account_balance/self.account_balance\n",
    "        self.reward_Q.append(self.reward)\n",
    "        self.reward_Q.append(self.reward)\n",
    "        self.reward_Q.append(self.reward)\n",
    "        \n",
    "        self.current_trade[\"reward\"]  = self.reward\n",
    "        # print(f\"DBG: enter trade_ current_trade {self.current_trade}\")\n",
    "        \n",
    "    def enter_idle_mode(self, ask_bid_lines_m1_short=None, ask_bid_lines_m5_short = None, ask_bid_lines_m1_long = None, ask_bid_lines_m5_long = None, current_image_m1_short=None, current_image_m5_short=None, current_image_m1_long=None, current_image_m5_long = None):\n",
    "        \n",
    "        self.current_trade[\"ask_bid_lines_m5_short\"] = ask_bid_lines_m5_short\n",
    "        self.current_trade[\"ask_bid_lines_m5_long\"] = ask_bid_lines_m5_long\n",
    "        self.current_trade[\"entry_price\"] = float(f\"{self.m5short.iloc[self.current_step]['Ask']}\")\n",
    "        self.current_trade[\"current_price\"] = float(f\"{self.m5short.iloc[self.current_step]['Ask']}\")\n",
    "        self.current_trade[\"position\"] = self.position\n",
    "        self.current_trade[\"reward\"] = 0\n",
    "        self.current_trade[\"timesteps\"] = 1\n",
    "        self.current_trade[\"profit\"] = -3\n",
    "        self.current_trade[\"parent_ask_bid_lines_m5_short\"] = None\n",
    "        self.current_trade[\"parent_ask_bid_lines_m5_long\"] = None\n",
    "        self.current_trade[\"balance\"] = self.account_balance\n",
    "        \n",
    "        self.current_trade[\"price_ma\"].append(0.10)\n",
    "        self.current_trade[\"price_ma\"].append(0.10)\n",
    "        self.current_trade[\"price_ma\"].append(0.10)\n",
    "        self.current_trade[\"price_ma\"].append(0.10)\n",
    "        \n",
    "        self.current_trade[\"reward_ma\"].append(0.00)\n",
    "        self.current_trade[\"reward_ma\"].append(0.00)\n",
    "        self.current_trade[\"reward_ma\"].append(0.00)\n",
    "        self.current_trade[\"reward_ma\"].append(0.00)\n",
    "        \n",
    "        current_image_m5_short = self.draw_buy_bar(current_image_m5_short, ask_bid_lines_m5_short, ask_bid_lines_m5_short, self.position, self.current_trade[\"profit\"], self.prev_trade)\n",
    "        current_image_m5_long = self.draw_buy_bar(current_image_m5_long, ask_bid_lines_m5_long, ask_bid_lines_m5_long, self.position, self.current_trade[\"profit\"], self.prev_trade)\n",
    "        self.trade_queue_m5_short.append(ask_bid_lines_m5_short)\n",
    "        self.trade_queue_m5_long.append(ask_bid_lines_m5_long)\n",
    "        self.image_queue_m5_short.append(current_image_m5_short)  \n",
    "        self.image_queue_m5_long.append(current_image_m5_long)\n",
    "        if self.env_draw:\n",
    "            self.draw_collection(self.image_queue_m5_short,self.image_queue_m5_long) \n",
    "            # self.draw_collection(self.image_queue_m5_short,self.image_queue_m5_long) \n",
    "        self.reward = 0#self.get_reward(self.current_trade)\n",
    "        self.current_trade[\"reward\"] = self.reward\n",
    "        # print(f\"DBG: enter idle_ current_trade {self.current_trade}\")\n",
    "    def hold_position(self, ask_bid_lines_m1_short=None, ask_bid_lines_m5_short = None, ask_bid_lines_m1_long = None, ask_bid_lines_m5_long = None, current_image_m1_short=None, current_image_m5_short=None, current_image_m1_long=None, current_image_m5_long = None):\n",
    "        \n",
    "        self.current_trade[\"current_price\"] = float(f\"{self.m5short.iloc[self.current_step]['Bid']}\") if self.position == \"buy\" else float(f\"{self.m5short.iloc[self.current_step]['Ask']}\")\n",
    "        self.current_trade[\"profit\"] = self.current_trade[\"current_price\"] - self.current_trade[\"entry_price\"] if self.position == \"buy\" else self.current_trade[\"entry_price\"] - self.current_trade[\"current_price\"]\n",
    "        self.current_trade[\"timesteps\"] += 1\n",
    "        self.current_trade[\"balance\"] = self.account_balance\n",
    "        self.current_trade[\"highest\"] = self.current_trade[\"profit\"] if self.current_trade[\"profit\"] > self.current_trade[\"highest\"] else self.current_trade[\"highest\"]\n",
    "        #self.current_trade[\"highest\"] -= 15\n",
    "\n",
    "        current_image_m5_short = self.draw_buy_bar(current_image_m5_short, self.current_trade[\"parent_ask_bid_lines_m5_short\"], ask_bid_lines_m5_short, self.position , self.current_trade[\"profit\"], self.prev_trade)\n",
    "        current_image_m5_long = self.draw_buy_bar(current_image_m5_long, self.current_trade[\"parent_ask_bid_lines_m5_long\"], ask_bid_lines_m5_long, self.position, self.current_trade[\"profit\"], self.prev_trade)\n",
    "        self.trade_queue_m5_short.append(ask_bid_lines_m5_short)\n",
    "        self.trade_queue_m5_long.append(ask_bid_lines_m5_long)\n",
    "        self.image_queue_m5_short.append(current_image_m5_short)\n",
    "        self.image_queue_m5_long.append(current_image_m5_long)\n",
    "        #fix price variance with ma for each trade\n",
    "        self.current_trade[\"price_ma\"].append(self.current_trade[\"profit\"])\n",
    "        \n",
    "        if len(self.current_trade[\"price_ma\"]) < 2:\n",
    "            if self.current_trade[\"profit\"] < 0 and self.current_trade > -10:\n",
    "                self.current_trade[\"price_ma\"].append(1)\n",
    "                self.current_trade[\"price_ma\"].append(1)\n",
    "                # self.current_trade[\"price_ma\"].append(1)\n",
    "                # self.current_trade[\"price_ma\"].append(1)\n",
    "                \n",
    "            elif self.current_trade[\"price_ma\"] > 0 and self.current_trade[\"profit\"] > 0:\n",
    "                self.current_trade[\"price_ma\"].append(self.current_trade[\"profit\"])\n",
    "                self.current_trade[\"price_ma\"].append(self.current_trade[\"profit\"])\n",
    "                # self.current_trade[\"price_ma\"].append(self.current_trade[\"profit\"])\n",
    "                # self.current_trade[\"price_ma\"].append(self.current_trade[\"profit\"])\n",
    "                # print(f\"DBG: hold_position {self.current_trade['price_ma']}\")\n",
    "        \n",
    "        if self.env_draw:\n",
    "            self.draw_collection(self.image_queue_m5_short,self.image_queue_m5_long) \n",
    "            # self.draw_collection(self.image_queue_m5_short,self.image_queue_m5_long) \n",
    "        if self.current_trade[\"profit\"] >= 0:\n",
    "            self.reward = 0#self.get_reward(self.current_trade)#(self.account_balance + self.current_trade[\"profit\"])/self.account_balance\n",
    "            self.current_trade[\"reward\"] = self.reward\n",
    "        else:\n",
    "            self.reward = 0#self.get_reward(self.current_trade)#-1 - (self.account_balance + self.current_trade[\"profit\"])/self.account_balance\n",
    "            self.current_trade[\"reward\"] = self.reward\n",
    "        # print(f\"DBG::hold positio   n_ current_trade {self.current_trade}\")\n",
    "        self.current_trade[\"reward_ma\"].append(self.reward)\n",
    "        self.reward = 0#sum(self.current_trade[\"reward_ma\"])/len(self.current_trade[\"reward_ma\"])\n",
    "        self.reward_Q.append(self.reward)\n",
    "        self.reward = sum(self.reward_Q)/len(self.reward_Q)\n",
    "        self.current_trade[\"reward\"] = self.reward\n",
    "\n",
    "        \n",
    "    \n",
    "    def close_position(self, ask_bid_lines_m1_short=None, ask_bid_lines_m5_short = None, ask_bid_lines_m1_long = None, ask_bid_lines_m5_long = None, current_image_m1_short=None, current_image_m5_short=None, current_image_m1_long=None, current_image_m5_long = None, prev_trade = None):\n",
    "        # print(f\"self.position {self.position}\")\n",
    "        self.current_trade[\"current_price\"] = float(f\"{self.m5short.iloc[self.current_step]['Bid']}\") if self.position == \"buy\" else float(f\"{self.m5short.iloc[self.current_step]['Ask']}\")\n",
    "        self.current_trade[\"profit\"] = self.current_trade[\"current_price\"] - self.current_trade[\"entry_price\"] if self.position == \"buy\" else self.current_trade[\"entry_price\"] - self.current_trade[\"current_price\"]\n",
    "        self.current_trade[\"highest\"] = self.current_trade[\"profit\"] if self.current_trade[\"profit\"] > self.current_trade[\"highest\"] else self.current_trade[\"highest\"]\n",
    "        #self.current_trade[\"highest\"] += 5\n",
    "\n",
    "        current_image_m5_short = self.draw_buy_bar(current_image_m5_short, self.parent_ask_bid_lines_m5_short, ask_bid_lines_m5_short, self.position, self.current_trade[\"profit\"] , prev_trade)\n",
    "        current_image_m5_long = self.draw_buy_bar(current_image_m5_long, self.parent_ask_bid_lines_m5_long, ask_bid_lines_m5_long, self.position, self.current_trade[\"profit\"], prev_trade)\n",
    "        self.trade_queue_m5_short.append(ask_bid_lines_m5_short)\n",
    "        self.trade_queue_m5_long.append(ask_bid_lines_m5_long)\n",
    "        self.image_queue_m5_short.append(current_image_m5_short)\n",
    "        self.image_queue_m5_long.append(current_image_m5_long)\n",
    "        self.account_balance += self.current_trade[\"profit\"]\n",
    "        #fix high variance for the price\n",
    "        self.current_trade[\"price_ma\"].append(self.current_trade[\"profit\"])\n",
    "        \n",
    "        if self.env_draw:\n",
    "            self.draw_collection(self.image_queue_m5_short,self.image_queue_m5_long) \n",
    "            # self.draw_collection(self.image_queue_m5_short,self.image_queue_m5_long) \n",
    "        if self.current_trade[\"profit\"] >= 0:\n",
    "            if self.current_trade[\"profit\"] * 80 < self.current_trade[\"highest\"]:\n",
    "                self.reward = 1#np.log(self.account_balance) - np.log(self.current_trade[\"highest\"]-self.current_trade[\"profit\"])\n",
    "            else:\n",
    "                self.reward = 1#np.log(self.account_balance)#self.get_reward(self.current_trade)##(self.account_balance + self.current_trade[\"profit\"])/self.account_balance\n",
    "        else:\n",
    "            self.reward =  -1#-np.log(2) - np.log(abs(self.current_trade[\"profit\"] - 20))#self.get_reward(self.current_trade)#-1 - (self.account_balance + self.current_trade[\"profit\"])/self.account_balance\n",
    "        #self.trade_queue, self.image_queue = self.reset_collection(self.trade_queue,self.image_queue)\n",
    "        # self.account_balance += self.current_trade[\"profit\"]\n",
    "        \n",
    "        self.current_trade[\"reward_ma\"].append(self.reward)\n",
    "        # self.reward = sum(self.current_trade[\"reward_ma\"])/len(self.current_trade[\"reward_ma\"])\n",
    "        self.current_trade[\"reward\"] = self.reward        \n",
    "        \n",
    "        self.current_trade[\"current_timestep\"] = self.current_step\n",
    "        self.current_trade[\"timesteps\"] += 1\n",
    "        self.current_trade[\"position\"] = \"close\"\n",
    "        self.current_trade[\"balance\"] = self.account_balance\n",
    "        self.current_trade[\"episode_profit\"] = self.episode_profit\n",
    "        self.position = \"close\"\n",
    "        self.parent_ask_bid = None\n",
    "        \n",
    "        # print(f\"DBG: close trade_ current_trade {self.current_trade}\")\n",
    "    def step(self, action):\n",
    "        self.reward = 0\n",
    "        self.done = 0\n",
    "        self.action = action\n",
    "        \n",
    "        ask_bid_lines_m5_short, current_image_m5_short = self.get_ask_bid_lines(f\"{self.m5short.iloc[self.current_step]['image_path']}\")\n",
    "        ask_bid_lines_m5_long, current_image_m5_long = self.get_ask_bid_lines(f\"{self.m5long.iloc[self.current_step]['image_path']}\")\n",
    "        ask_bid_lines_m5_short[\"key\"] = self.current_step\n",
    "        ask_bid_lines_m5_long[\"key\"] = self.current_step\n",
    "        \n",
    "        \n",
    "        \n",
    "        if self.position == \"close\":\n",
    "            if action == 1:\n",
    "                self.position = \"buy\"\n",
    "                self.prev_trade.append(1)\n",
    "                self.parent_ask_bid_lines_m5_short = ask_bid_lines_m5_short\n",
    "                self.parent_ask_bid_lines_m5_long = ask_bid_lines_m5_long\n",
    "                \n",
    "                self.enter_trade(ask_bid_lines_m5_short = ask_bid_lines_m5_short, ask_bid_lines_m5_long = ask_bid_lines_m5_long, current_image_m5_short = current_image_m5_short, current_image_m5_long = current_image_m5_long)\n",
    "            elif action == 2:\n",
    "                self.position = \"sell\"\n",
    "                self.prev_trade.append(2)\n",
    "                self.parent_ask_bid_lines_m5_short = ask_bid_lines_m5_short\n",
    "                self.parent_ask_bid_lines_m5_long = ask_bid_lines_m5_long\n",
    "                self.enter_trade(ask_bid_lines_m5_short = ask_bid_lines_m5_short, ask_bid_lines_m5_long = ask_bid_lines_m5_long, current_image_m5_short = current_image_m5_short, current_image_m5_long = current_image_m5_long)\n",
    "            elif action == 0:\n",
    "                self.position = \"close\"\n",
    "                self.prev_trade.append(0)\n",
    "                self.parent_ask_bid_lines_m5_short = ask_bid_lines_m5_short\n",
    "                self.parent_ask_bid_lines_m5_long = ask_bid_lines_m5_long\n",
    "                self.enter_idle_mode(ask_bid_lines_m5_short = ask_bid_lines_m5_short, ask_bid_lines_m5_long = ask_bid_lines_m5_long, current_image_m5_short = current_image_m5_short, current_image_m5_long = current_image_m5_long)\n",
    "        elif self.position == \"buy\":\n",
    "            if action == 1:\n",
    "                self.prev_trade.append(1)\n",
    "                self.hold_position(ask_bid_lines_m5_short = ask_bid_lines_m5_short, ask_bid_lines_m5_long = ask_bid_lines_m5_long, current_image_m5_short = current_image_m5_short, current_image_m5_long = current_image_m5_long)\n",
    "            elif action in [2]:\n",
    "                self.prev_trade.append(0)\n",
    "                self.close_position(ask_bid_lines_m5_short = ask_bid_lines_m5_short, ask_bid_lines_m5_long = ask_bid_lines_m5_long, current_image_m5_short = current_image_m5_short, current_image_m5_long = current_image_m5_long, prev_trade = self.prev_trade)                \n",
    "                #fix standard limit and standard balance on chart \n",
    "                if self.account_balance > int((2/3) * self.chart_decorator.balance_limit) + self.chart_decorator.standard_balance:\n",
    "                    # print(\"\\n\\nReseting the account balance in closing buy trade using 2\\n\\n the if statement\\n\\n\\n\")\n",
    "                    self.chart_decorator.standard_balance = self.account_balance\n",
    "                # elif self.chart_decorator.standard_balance * (2/3) > self.account_balance :\n",
    "                    # print(\"\\n\\nReseting the account balance in closing buy trade using 2\\n\\n the elif statement\\n\\n\\n\")\n",
    "                    # self.chart_decorator.standard_balance -= self.chart_decorator.balance_limit  \n",
    "                # self.reward = (self.account_balance + self.current_trade[\"profit\"])/1000\n",
    "                # self.current_trade[\"reward\"] = (self.account_balance + self.current_trade[\"profit\"])/1000\n",
    "                    \n",
    "            elif action in [0]:\n",
    "                self.prev_trade.append(0)\n",
    "                self.close_position(ask_bid_lines_m5_short = ask_bid_lines_m5_short, ask_bid_lines_m5_long = ask_bid_lines_m5_long, current_image_m5_short = current_image_m5_short, current_image_m5_long=current_image_m5_long, prev_trade = self.prev_trade)   \n",
    "                \n",
    "                #fix standard limit and standard balance on chart\n",
    "                if self.account_balance > int((2/3) * self.chart_decorator.balance_limit) + self.chart_decorator.standard_balance:\n",
    "                    # print(\"\\n\\nReseting the account balance in closing buy trade using 0\\n\\n the if statement\\n\\n\\n\")\n",
    "                    self.chart_decorator.standard_balance = self.account_balance\n",
    "                # elif self.chart_decorator.standard_balance * (2/3) > self.account_balance :\n",
    "                    # print(\"\\n\\nReseting the account balance in closing buy trade using 0\\n\\n the elif statement\\n\\n\\n\")\n",
    "                    # self.chart_decorator.standard_balance -= self.chart_decorator.balance_limit  \n",
    "            \n",
    "        elif self.position == \"sell\":\n",
    "            if action == 2:\n",
    "                self.prev_trade.append(2)\n",
    "                self.hold_position(ask_bid_lines_m5_short = ask_bid_lines_m5_short, ask_bid_lines_m5_long = ask_bid_lines_m5_long, current_image_m5_short = current_image_m5_short, current_image_m5_long = current_image_m5_long)\n",
    "            #this reward is to penalize it not to change the position without closing the position\n",
    "            elif action in [1]:\n",
    "                self.prev_trade.append(0)\n",
    "                self.close_position(ask_bid_lines_m5_short = ask_bid_lines_m5_short, ask_bid_lines_m5_long = ask_bid_lines_m5_long, current_image_m5_short = current_image_m5_short, current_image_m5_long = current_image_m5_long, prev_trade = self.prev_trade)\n",
    "                #fix standard limit and standard balance on chart\n",
    "                if self.account_balance > int((2/3) * self.chart_decorator.balance_limit) + self.chart_decorator.standard_balance:\n",
    "                    # print(\"\\n\\nReseting the account balance in closing sell trade using 1\\n\\n the if statement\\n\\n\\n\")\n",
    "                    \n",
    "                    self.chart_decorator.standard_balance = self.account_balance\n",
    "                # elif self.chart_decorator.standard_balance * (2/3) > self.account_balance :\n",
    "                    # print(\"\\n\\nReseting the account balance in closing sell trade using 1\\n\\n the elif statement\\n\\n\\n\")\n",
    "                    # self.chart_decorator.standard_balance -= self.chart_decorator.balance_limit  \n",
    "                # self.reward = (self.account_balance + self.current_trade[\"profit\"])/1000\n",
    "                # self.current_trade[\"reward\"] = (self.account_balance + self.current_trade[\"profit\"])/1000\n",
    "            elif action == 0:\n",
    "                self.prev_trade.append(0)\n",
    "                self.close_position(ask_bid_lines_m5_short = ask_bid_lines_m5_short, ask_bid_lines_m5_long = ask_bid_lines_m5_long, current_image_m5_short = current_image_m5_short, current_image_m5_long = current_image_m5_long, prev_trade = self.prev_trade)\n",
    "                # self.prev_trade.append(0)\n",
    "                #fix standard limit and standard balance on chart\n",
    "                if self.account_balance > int((2/3) * self.chart_decorator.balance_limit) + self.chart_decorator.standard_balance:\n",
    "                    # print(\"\\n\\nReseting the account balance in closing sell trade using 0\\n\\n the if statement\\n\\n\\n\")\n",
    "                    # print(f\"Account Balance == {self.account_balance} chart decorator standard_balance {self.chart_decorator.standard_balance}\")\n",
    "                    self.chart_decorator.standard_balance = self.account_balance\n",
    "                    # print(f\"updated standard balance == {self.chart_decorator.standard_balance}\")\n",
    "                # elif self.chart_decorator.standard_balance * (2/3) > self.account_balance :\n",
    "                    # print(\"\\n\\nReseting the account balance in closing sell trade using 0\\n\\n the elif statement\\n\\n\\n\")\n",
    "                    # self.chart_decorator.standard_balance -= self.chart_decorator.balance_limit \n",
    "        self.current_step+= self.step_size\n",
    "        print(f\"self.current_step {self.current_step}\")\n",
    "        next_state = self.get_obs()\n",
    "        # if  self.account_balance > self.old_balance + 100:\n",
    "        #     self.old_balance = self.account_balance\n",
    "        \n",
    "        if self.account_balance < 950 or self.current_step > 9800:#:self.old_balance:\n",
    "            self.done = 1\n",
    "        # if self.current_step > 9800:\n",
    "        #     self.done = 1\n",
    "        # print(self.trade_queue)\n",
    "        \n",
    "        return next_state, self.reward, self.done, self.current_trade\n",
    "    \n",
    "    def get_obs(self):\n",
    "        # stacked_obs =   np.expand_dims(np.stack([\n",
    "        #                 #np.asarray(self.image_queue_m5_long[1])/255,\n",
    "        #                 #np.asarray(self.image_queue_m5_long[2])/255,\n",
    "        #                 np.asarray(self.image_queue_m5_long[3]),\n",
    "        #                 # np.asarray(self.image_queue_m5_short[0])/255, \n",
    "        #                 # np.asarray(self.image_queue_m5_short[1])/255, \n",
    "        #                 # np.asarray(self.image_queue_m5_short[2])/255, \n",
    "        #                 # np.asarray(self.image_queue_m5_short[3])/255,\n",
    "        #                 # np.asarray(self.image_queue[0])/255, \n",
    "        #                 # np.asarray(self.image_queue[1])/255, \n",
    "        #                 # np.asarray(self.image_queue[2])/255, \n",
    "        #                 # np.asarray(self.image_queue[3])/255\n",
    "        #     ]), axis=0)\n",
    "        # stacked_obs_m5 = np.expand_dims(np.stack([np.asarray(self.image_queue_m5[0])/255, np.asarray(self.image_queue_m5[1])/255, np.asarray(self.image_queue_m5[2])/255, np.asarray(self.image_queue_m5[3])/255]), axis=0)\n",
    "        # stacked = np.concatenate([stacked_obs_m5, stacked_obs], axis=0)\n",
    "        return np.asarray(self.image_queue_m5_long[3])\n",
    "        # return np.expand_dims(np.asarray(self.image_queue_m5_long[3]), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9d4ed-d045-40b5-ba6f-66d0501c4052",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "408be9d6-4215-48b1-bd4f-f4e2b96a3b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import ale_py\n",
    "from ray.rllib.env.wrappers.atari_wrappers import wrap_deepmind\n",
    "from ncps.tf import CfC\n",
    "import numpy as np\n",
    "from ncps.datasets.tf import AtariCloningDatasetTF\n",
    "\n",
    "\n",
    "# Not used in this example\n",
    "class ConvBlock(tf.keras.models.Sequential):\n",
    "    def __init__(self):\n",
    "        super(ConvBlock, self).__init__(\n",
    "            [\n",
    "                tf.keras.Input((84, 84, 4)),\n",
    "                tf.keras.layers.Lambda(\n",
    "                    lambda x: tf.cast(x, tf.float32) / 255.0\n",
    "                ),  # normalize input\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    64, 5, padding=\"same\", activation=\"relu\", strides=2\n",
    "                ),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    128, 5, padding=\"same\", activation=\"relu\", strides=2\n",
    "                ),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    128, 5, padding=\"same\", activation=\"relu\", strides=2\n",
    "                ),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    256, 5, padding=\"same\", activation=\"relu\", strides=2\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class ImpalaConvLayer(tf.keras.models.Sequential):\n",
    "    def __init__(self, filters, kernel_size, strides, padding=\"valid\", use_bias=False):\n",
    "        super(ImpalaConvLayer, self).__init__(\n",
    "            [\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    strides=strides,\n",
    "                    padding=padding,\n",
    "                    use_bias=use_bias,\n",
    "                    kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
    "                        scale=2.0, mode=\"fan_out\", distribution=\"truncated_normal\"\n",
    "                    ),\n",
    "                ),\n",
    "                tf.keras.layers.BatchNormalization(momentum=0.99, epsilon=0.001),\n",
    "                tf.keras.layers.ReLU(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class ImpalaConvBlock(tf.keras.models.Sequential):\n",
    "    def __init__(self):\n",
    "        super(ImpalaConvBlock, self).__init__(\n",
    "            [\n",
    "                ImpalaConvLayer(filters=16, kernel_size=8, strides=4),\n",
    "                ImpalaConvLayer(filters=32, kernel_size=4, strides=2),\n",
    "                ImpalaConvLayer(filters=32, kernel_size=3, strides=1),\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(units=256, activation=\"relu\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "class ConvCfC(tf.keras.Model):\n",
    "    def __init__(self, n_actions):\n",
    "        super().__init__()\n",
    "        self.conv_block = ImpalaConvBlock()\n",
    "        self.td_conv = tf.keras.layers.TimeDistributed(self.conv_block)\n",
    "        self.rnn = CfC(64, return_sequences=True, return_state=True)\n",
    "        self.linear = tf.keras.layers.Dense(n_actions)\n",
    "\n",
    "    def get_initial_states(self, batch_size=1):\n",
    "        return self.rnn.cell.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x, training=None, **kwargs):\n",
    "        has_hx = isinstance(x, list) or isinstance(x, tuple)\n",
    "        initial_state = None\n",
    "        if has_hx:\n",
    "            # additional inputs are passed as a tuple\n",
    "            x, initial_state = x\n",
    "\n",
    "        x = self.td_conv(x, training=training)\n",
    "        x, next_state = self.rnn(x, initial_state=initial_state)\n",
    "        x = self.linear(x)\n",
    "        if has_hx:\n",
    "            return (x, next_state)\n",
    "        return x\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_closed_loop(model, env, num_episodes=None):\n",
    "    obs = env.reset()\n",
    "    hx = model.get_initial_states()\n",
    "    returns = []\n",
    "    total_reward = 0\n",
    "    while True:\n",
    "        # add batch and time dimension (with a single element in each)\n",
    "        plt.imshow(obs)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        obs = np.expand_dims(np.expand_dims(obs, 0), 0)\n",
    "        pred, hx = model.predict((obs, hx), verbose=0)\n",
    "        action = pred[0, 0].argmax()\n",
    "        # remove time and batch dimension -> then argmax\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        print(\"reward \", r)\n",
    "        total_reward += r\n",
    "        if done:\n",
    "            returns.append(total_reward)\n",
    "            total_reward = 0\n",
    "            obs = env.reset()\n",
    "            hx = model.get_initial_states()\n",
    "            # Reset RNN hidden states when episode is over\n",
    "            if num_episodes is not None:\n",
    "                # Count down the number of episodes\n",
    "                num_episodes = num_episodes - 1\n",
    "                if num_episodes == 0:\n",
    "                    return returns\n",
    "\n",
    "\n",
    "class ClosedLoopCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model, env):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.env = env\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        r = run_closed_loop(self.model, self.env, num_episodes=10)\n",
    "        print(f\"\\nEpoch {epoch} return: {np.mean(r):0.2f} +- {np.std(r):0.2f}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # env = gym.make(\"ALE/Breakout-v5\")\n",
    "#     # env = wrap_deepmind(env)\n",
    "\n",
    "#     # data = AtariCloningDatasetTF(\"breakout\")\n",
    "#     # batch_size = 32\n",
    "#     # trainloader = data.get_dataset(32, split=\"train\")\n",
    "#     # valloader = data.get_dataset(32, split=\"val\")\n",
    "\n",
    "#     model = ConvCfC(3)\n",
    "\n",
    "#     model.compile(\n",
    "#         loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#         optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "#         metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "#     )\n",
    "    \n",
    "#     model.build((None, None, 224, 224, 3))\n",
    "#     model.summary()\n",
    "#     # model.load_weights(\"models/breakout/model_ALE_Breakout_Impala.h5\")\n",
    "#     model.fit(\n",
    "#         (x,y),\n",
    "#         epochs=10,\n",
    "#         validation_data=(x,y),\n",
    "#         callbacks=[ClosedLoopCallback(model, env)],\n",
    "#     )\n",
    "\n",
    "    # Visualize Atari game and play endlessly\n",
    "    # env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\")\n",
    "    # env = wrap_deepmind(env)\n",
    "    # run_closed_loop(model, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f82757-f053-4af9-a343-ab4fd558ef24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac36a3e1-dd8b-4da5-856d-1e16e8a3f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.array(X)\n",
    "# y = np.array(y)\n",
    "\n",
    "# # Reshape x to have the shape (num_samples, width, height, channels)\n",
    "# x = x.reshape(-1, 224, 224, 3)\n",
    "# x = np.expand_dims(x,axis=0)\n",
    "# x = np.expand_dims(x,axis=0)\n",
    "# y = np.expand_dims(y,axis=0)\n",
    "# y = np.expand_dims(y,axis=0)\n",
    "\n",
    "# print(y.shape)\n",
    "# x.shape\n",
    "# # (1, 1, 32, 84, 84, 4)\n",
    "# #(1, 1, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded1296e-e059-4ed8-8887-ec7e83498abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "m5short_step_4 = pd.read_csv(\"final_complete_m5short_timestep_4.csv\")\n",
    "m1short_step_4 = pd.read_csv(\"final_complete_m1short_timestep_4.csv\")\n",
    "m1long_step_4 = pd.read_csv(\"final_complete_m1long_timestep_4.csv\")\n",
    "m5long_step_4 = pd.read_csv(\"final_complete_m5long_timestep_4.csv\")\n",
    "\n",
    "chart_decorator = ChartDecorator()\n",
    "env = ForexCustomEnv(m1short_step_4 ,m5short_step_4, m1long_step_4, m5long_step_4,chart_decorator)\n",
    "env.env_draw = 1\n",
    "counter = 4\n",
    "state, reward, done, _ =  env.reset(counter, step= 50)\n",
    "a = 0\n",
    "X = []\n",
    "Y = []\n",
    "while not done:\n",
    "    if counter <= 28:\n",
    "        a = 2\n",
    "    if counter == 29:\n",
    "        a = 0\n",
    "    if counter >= 30:\n",
    "        a = 1\n",
    "    if counter == 71:\n",
    "        a = 0\n",
    "    if counter >= 72 and counter < 99:\n",
    "        a = 2\n",
    "    if counter == 96:\n",
    "        a = 0\n",
    "    if counter >= 97 and counter <= 109:\n",
    "        a = 1\n",
    "    if counter == 110:\n",
    "        a = 0\n",
    "    if counter >= 111 and counter <= 175:\n",
    "        a = 2\n",
    "    if counter >= 176:\n",
    "        a = 0\n",
    "\n",
    "    if counter >= 177 and counter <= 184:\n",
    "        a = 1\n",
    "    if counter == 185:\n",
    "        a = 0\n",
    "    if counter >= 186:\n",
    "        a = 2\n",
    "    if counter == 199:\n",
    "        a = 0\n",
    "    # if counter >= 114 or counter <= 151:\n",
    "    #     a = 2\n",
    "    # if counter >= 152:\n",
    "    #     a = 0\n",
    "    action, pred = a, []\n",
    "    next_state, reward, done, current_trade = env.step(action)\n",
    "    print(counter)#,action, reward, done)\n",
    "    # print(state)\n",
    "    # print(f\"next state {next_state}\")\n",
    "    counter += 1\n",
    "    # print(state.shape)\n",
    "    X.append(state)\n",
    "    Y.append(action)\n",
    "\n",
    "    state = next_state\n",
    "    # break\n",
    "    if counter > 250:\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "68871f3f-e477-4ca2-9081-bb69ffabe87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.array(X)\n",
    "y = np.array(Y)\n",
    "\n",
    "# Reshape x to have the shape (num_samples, width, height, channels)\n",
    "x = x.reshape(-1, 224, 224, 3)\n",
    "x = np.expand_dims(x,axis=0)\n",
    "x = np.expand_dims(x,axis=0)\n",
    "y = np.expand_dims(y,axis=0)\n",
    "y = np.expand_dims(y,axis=0)\n",
    "\n",
    "\n",
    "# Reshape x to have the shape (num_samples, width, height, channels)\n",
    "# x = x.reshape(-1, 224, 224, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e1cea4b-fea9-4958-b6bb-58b7e730366a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.random.random(1)*10) >= 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "976c9c6f-4841-4ec8-98fc-12a8bcd4877c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(int(np.random.random(1)*10) >= 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c1409c7-ab63-4ccd-bdb4-d0b6ff93f9e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([True,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "eab904f6-48ab-4d4d-8da0-74d43034492c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(1, 196, 224, 224, 3), dtype=tf.uint8, name=None), TensorSpec(shape=(1, 196), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_dataset(x, y):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "x = tf.convert_to_tensor(x)\n",
    "y = tf.convert_to_tensor(y)\n",
    "# # Determine the dataset size based on the first dimension of x or y\n",
    "dataset_size = tf.shape(x)[0] if tf.shape(x)[0] > tf.shape(y)[0] else tf.shape(y)[0]\n",
    "\n",
    "# Create a range of indices for batching\n",
    "indices = tf.range(dataset_size)\n",
    "\n",
    "# Create a dataset from the indices\n",
    "dataset = tf.data.Dataset.from_tensor_slices(indices)\n",
    "\n",
    "# Apply a map function to retrieve elements from x and y based on the indices\n",
    "dataset = dataset.map(lambda i: (x[i], y[i]))\n",
    "\n",
    "# Prefetch the dataset for optimization\n",
    "prefetched_dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "element_spec = prefetched_dataset\n",
    "\n",
    "element_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1033788a-c818-4ebc-81a5-a02cbdcd29b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvCfC(3)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(0.0001),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.build((None, None, 224, 224, 3))\n",
    "model.summary()\n",
    "# model.load_weights(\"models/breakout/model_ALE_Breakout_Impala.h5\")\n",
    "model.fit(\n",
    "    element_spec,\n",
    "    epochs=50,\n",
    "    validation_data=element_spec,\n",
    "    #callbacks=[ClosedLoopCallback(model, env)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b147066f-c553-4e2a-bcee-28728697be00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_profits = 0\n",
    "def run_closed_loop(model, env, total_profits,num_episodes=None):\n",
    "    counter = 4\n",
    "    env.env_draw = 1\n",
    "    obs,_,_,current_trade = env.reset(counter, step=50)\n",
    "    hx = model.get_initial_states()\n",
    "    returns = []\n",
    "    total_reward = 0\n",
    "    \n",
    "    while True:\n",
    "        # add batch and time dimension (with a single element in each)\n",
    "        # plt.imshow(obs)\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        # print(obs.shape)\n",
    "        obs = tf.convert_to_tensor(obs)\n",
    "        pred, hx = model.predict((obs, hx), verbose=0)\n",
    "        action = pred[0, 0].argmax()\n",
    "        # remove time and batch dimension -> then argmax\n",
    "        obs, r, done, current_trade = env.step(action)\n",
    "        # print(\"reward \", r)\n",
    "        total_reward += r\n",
    "        counter+=1\n",
    "        if current_trade[\"position\"] == \"close\":\n",
    "            print(\"close found\")\n",
    "            total_profits += current_trade[\"profit\"]\n",
    "        print(counter , action, total_profits, current_trade[\"current_step\"]) \n",
    "        if done:\n",
    "            returns.append(total_reward)\n",
    "            total_reward = 0\n",
    "            obs = env.reset(4, step = 50)\n",
    "            hx = model.get_initial_states()\n",
    "            # Reset RNN hidden states when episode is over\n",
    "            if num_episodes is not None:\n",
    "                # Count down the number of episodes\n",
    "                num_episodes = num_episodes - 1\n",
    "                if num_episodes == 0:\n",
    "                    return returns\n",
    "chart_decorator = ChartDecorator()\n",
    "env = ForexCustomEnv(m1short_step_4 ,m5short_step_4, m1long_step_4, m5long_step_4,chart_decorator)\n",
    "total_profits = run_closed_loop(model, env, total_profits, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a563c5f4-50da-4628-a7db-ae504aa86f12",
   "metadata": {},
   "source": [
    "Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0809f51c-504c-4501-bd74-54e299bb2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from ray.tune.registry import register_env\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "import time\n",
    "import ale_py\n",
    "from ray.rllib.env.wrappers.atari_wrappers import wrap_deepmind\n",
    "import numpy as np\n",
    "\n",
    "from ray.rllib.models.modelv2 import ModelV2\n",
    "from ray.rllib.models.tf.recurrent_net import RecurrentNetwork\n",
    "from ray.rllib.utils.annotations import override\n",
    "\n",
    "import tensorflow as tf\n",
    "from ncps.tf import CfC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "70a8c832-0415-43b8-a8c2-c645d5cf1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCfCModel(RecurrentNetwork):\n",
    "    \"\"\"Example of using the Keras functional API to define a RNN model.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_space,\n",
    "        action_space,\n",
    "        num_outputs,\n",
    "        model_config,\n",
    "        name,\n",
    "        cell_size=64,\n",
    "    ):\n",
    "        super(ConvCfCModel, self).__init__(\n",
    "            obs_space, action_space, num_outputs, model_config, name\n",
    "        )\n",
    "        self.cell_size = cell_size\n",
    "\n",
    "        # Define input layers\n",
    "        input_layer = tf.keras.layers.Input(\n",
    "            shape=(None, obs_space.shape[0] * obs_space.shape[1] * obs_space.shape[2]),\n",
    "            name=\"inputs\",\n",
    "        )\n",
    "        state_in_h = tf.keras.layers.Input(shape=(cell_size,), name=\"h\")\n",
    "        seq_in = tf.keras.layers.Input(shape=(), name=\"seq_in\", dtype=tf.int32)\n",
    "\n",
    "        # Preprocess observation with a hidden layer and send to CfC\n",
    "        self.conv_block = tf.keras.models.Sequential(\n",
    "            [\n",
    "                tf.keras.Input(\n",
    "                    (obs_space.shape[0] * obs_space.shape[1] * obs_space.shape[2])\n",
    "                ),  # batch dimension is implicit\n",
    "                tf.keras.layers.Lambda(\n",
    "                    lambda x: tf.cast(x, tf.float32) / 255.0\n",
    "                ),  # normalize input\n",
    "                tf.keras.layers.Reshape(\n",
    "                    (obs_space.shape[0], obs_space.shape[1], obs_space.shape[2])\n",
    "                ),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    64, 5, padding=\"same\", activation=\"relu\", strides=2\n",
    "                ),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    128, 5, padding=\"same\", activation=\"relu\", strides=2\n",
    "                ),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    128, 5, padding=\"same\", activation=\"relu\", strides=2\n",
    "                ),\n",
    "                tf.keras.layers.Conv2D(\n",
    "                    256, 5, padding=\"same\", activation=\"relu\", strides=2\n",
    "                ),\n",
    "                tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            ]\n",
    "        )\n",
    "        self.td_conv = tf.keras.layers.TimeDistributed(self.conv_block)\n",
    "\n",
    "        dense1 = self.td_conv(input_layer)\n",
    "        cfc_out, state_h = CfC(\n",
    "            cell_size, return_sequences=True, return_state=True, name=\"cfc\"\n",
    "        )(\n",
    "            inputs=dense1,\n",
    "            mask=tf.sequence_mask(seq_in),\n",
    "            initial_state=[state_in_h],\n",
    "        )\n",
    "\n",
    "        # Postprocess CfC output with another hidden layer and compute values\n",
    "        logits = tf.keras.layers.Dense(\n",
    "            self.num_outputs, activation=tf.keras.activations.linear, name=\"logits\"\n",
    "        )(cfc_out)\n",
    "        values = tf.keras.layers.Dense(1, activation=None, name=\"values\")(cfc_out)\n",
    "\n",
    "        # Create the RNN model\n",
    "        self.rnn_model = tf.keras.Model(\n",
    "            inputs=[input_layer, seq_in, state_in_h],\n",
    "            outputs=[logits, values, state_h],\n",
    "        )\n",
    "        self.rnn_model.summary()\n",
    "\n",
    "    @override(RecurrentNetwork)\n",
    "    def forward_rnn(self, inputs, state, seq_lens):\n",
    "        model_out, self._value_out, h = self.rnn_model([inputs, seq_lens] + state)\n",
    "        return model_out, [h]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def get_initial_state(self):\n",
    "        return [\n",
    "            np.zeros(self.cell_size, np.float32),\n",
    "        ]\n",
    "\n",
    "    @override(ModelV2)\n",
    "    def value_function(self):\n",
    "        return tf.reshape(self._value_out, [-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5595b133-5178-4aef-9950-b0170e57d5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelCatalog.register_custom_model(\"cfc\", ConvCfCModel)\n",
    "\n",
    "def env_creator(x):\n",
    "    m5short_step_4 = pd.read_csv(\"final_complete_m5short_timestep_4.csv\")\n",
    "    m1short_step_4 = pd.read_csv(\"final_complete_m1short_timestep_4.csv\")\n",
    "    m1long_step_4 = pd.read_csv(\"final_complete_m1long_timestep_4.csv\")\n",
    "    m5long_step_4 = pd.read_csv(\"final_complete_m5long_timestep_4.csv\")\n",
    "    chart_decorator = ChartDecorator()\n",
    "    return ForexCustomEnv(m1short_step_4 ,m5short_step_4, m1long_step_4, m5long_step_4,chart_decorator)\n",
    "register_env(\"forex_env\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfb097-cea2-48ba-87a5-593c26870d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "04c5c125-780b-45f8-88be-98845feeb08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 23:21:17,367\tINFO algorithm.py:2292 -- Executing eagerly (framework='tf2'), with eager_tracing=False. For production workloads, make sure to set eager_tracing=True  in order to match the speed of tf-static-graph (framework='tf'). For debugging purposes, `eager_tracing=False` is the best choice.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m 2023-05-21 23:21:26,511\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=42370, ip=127.0.1.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f57d6dace50>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m     raise ValueError(\"Observation space must be a gym.space\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m ValueError: Observation space must be a gym.space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=42370, ip=127.0.1.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f57d6dace50>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 497, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m     check_env(self.env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 88, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 77, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m     check_gym_environments(env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m     raise ValueError(\"Observation space must be a gym.space\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m ValueError: Observation space must be a gym.space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42370)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior by setting `disable_env_checking=True` in your environment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([env]).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m 2023-05-21 23:21:26,576\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=42369, ip=127.0.1.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fdba7a34e80>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m     raise ValueError(\"Observation space must be a gym.space\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m ValueError: Observation space must be a gym.space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=42369, ip=127.0.1.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fdba7a34e80>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 497, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m     check_env(self.env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 88, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 77, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m     check_gym_environments(env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m     raise ValueError(\"Observation space must be a gym.space\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m ValueError: Observation space must be a gym.space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42369)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior by setting `disable_env_checking=True` in your environment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([env]).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m 2023-05-21 23:21:26,589\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=42371, ip=127.0.1.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe8cb418e50>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m     raise ValueError(\"Observation space must be a gym.space\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m ValueError: Observation space must be a gym.space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=42371, ip=127.0.1.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fe8cb418e50>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 497, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m     check_env(self.env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 88, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 77, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m     check_gym_environments(env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m     raise ValueError(\"Observation space must be a gym.space\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m ValueError: Observation space must be a gym.space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42371)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior by setting `disable_env_checking=True` in your environment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([env]).\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m 2023-05-21 23:21:26,594\tERROR worker.py:763 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=42368, ip=127.0.1.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f99d58dcdf0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m     raise ValueError(\"Observation space must be a gym.space\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m ValueError: Observation space must be a gym.space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m During handling of the above exception, another exception occurred:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=42368, ip=127.0.1.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f99d58dcdf0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 497, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m     check_env(self.env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 88, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m     raise ValueError(\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m ValueError: Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 77, in check_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m     check_gym_environments(env)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m   File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m     raise ValueError(\"Observation space must be a gym.space\")\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m ValueError: Observation space must be a gym.space\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=42368)\u001b[0m The above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior by setting `disable_env_checking=True` in your environment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([env]).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 77, in check_env\n    check_gym_environments(env)\n  File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n    raise ValueError(\"Observation space must be a gym.space\")\nValueError: Observation space must be a gym.space\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior by setting `disable_env_checking=True` in your environment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([env]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:524\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# WorkerSet creation possibly fails, if some (remote) workers cannot\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# be initialized properly (due to some errors in the RolloutWorker's\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# constructor).\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:139\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidate_workers_after_construction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:490\u001b[0m, in \u001b[0;36mWorkerSet.add_workers\u001b[0;34m(self, num_workers, validate)\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate:\n\u001b[0;32m--> 490\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_worker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_healthy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py:620\u001b[0m, in \u001b[0;36mWorkerSet.foreach_worker\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    619\u001b[0m     local_result \u001b[38;5;241m=\u001b[39m [func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_worker())]\n\u001b[0;32m--> 620\u001b[0m remote_results \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m local_result \u001b[38;5;241m+\u001b[39m remote_results\n",
      "File \u001b[0;32m~/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/_private/worker.py:2291\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2290\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2291\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   2293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=42370, ip=127.0.1.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f57d6dace50>)\n  File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n    raise ValueError(\"Observation space must be a gym.space\")\nValueError: Observation space must be a gym.space\n\nDuring handling of the above exception, another exception occurred:\n\n\u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=42370, ip=127.0.1.1, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f57d6dace50>)\n  File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 497, in __init__\n    check_env(self.env)\n  File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 88, in check_env\n    raise ValueError(\nValueError: Traceback (most recent call last):\n  File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 77, in check_env\n    check_gym_environments(env)\n  File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n    raise ValueError(\"Observation space must be a gym.space\")\nValueError: Observation space must be a gym.space\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior by setting `disable_env_checking=True` in your environment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([env]).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 52\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# register_env(\"atari_env\", lambda env_config: wrap_deepmind(gym.make(args.env)))\u001b[39;00m\n\u001b[1;32m     17\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlog_level\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINFO\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonitor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     50\u001b[0m }\n\u001b[0;32m---> 52\u001b[0m algo \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrl_ckpt/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;241m.\u001b[39menv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcont \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:414\u001b[0m, in \u001b[0;36mAlgorithm.__init__\u001b[0;34m(self, config, env, logger_creator, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m     }\n\u001b[1;32m    412\u001b[0m }\n\u001b[0;32m--> 414\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;66;03m# Check, whether `training_iteration` is still a tune.Trainable property\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# and has not been overridden by the user in the attempt to implement the\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;66;03m# algos logic (this should be done now inside `training_step`).\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/tune/trainable/trainable.py:161\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, custom_syncer, sync_timeout)\u001b[0m\n\u001b[1;32m    159\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mget_node_ip_address()\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py:549\u001b[0m, in \u001b[0;36mAlgorithm.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RayActorError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;66;03m# In case of an actor (remote worker) init failure, the remote worker\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;66;03m# may still exist and will be accessible, however, e.g. calling\u001b[39;00m\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;66;03m# its `sample.remote()` would result in strange \"property not found\"\u001b[39;00m\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# errors.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mactor_init_failed:\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;66;03m# Raise the original error here that the RolloutWorker raised\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;66;03m# during its construction process. This is to enforce transparency\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;66;03m# - e.args[0].args[2]: The original Exception (e.g. a ValueError due\u001b[39;00m\n\u001b[1;32m    548\u001b[0m         \u001b[38;5;66;03m# to a config mismatch) thrown inside the actor.\u001b[39;00m\n\u001b[0;32m--> 549\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;66;03m# In any other case, raise the RayActorError as-is.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 77, in check_env\n    check_gym_environments(env)\n  File \"/home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py\", line 140, in check_gym_environments\n    raise ValueError(\"Observation space must be a gym.space\")\nValueError: Observation space must be a gym.space\n\nThe above error has been found in your environment! We've added a module for checking your custom environments. It may cause your experiment to fail if your environment is not set up correctly. You can disable this behavior by setting `disable_env_checking=True` in your environment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env([env])."
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running\")\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--env\", type=str, default=\"ForexCustom\")\n",
    "    # parser.add_argument(\"--cont\", default=\"\")\n",
    "    # parser.add_argument(\"--render\", action=\"store_true\")\n",
    "    # parser.add_argument(\"--hours\", default=2, type=int)\n",
    "    # args = parser.parse_args()\n",
    "    class Args:\n",
    "        pass\n",
    "    args = Args()\n",
    "    args.env = \"forex_env\"\n",
    "    args.cont = \"\"\n",
    "    args.render = \"\"\n",
    "    args.hours = 2\n",
    "    # register_env(\"atari_env\", lambda env_config: wrap_deepmind(gym.make(args.env)))\n",
    "    config = {\n",
    "        \"log_level\":\"INFO\",\n",
    "        \"monitor\":True,\n",
    "        \"env\": \"forex_env\",\n",
    "        \"preprocessor_pref\": None,\n",
    "        \"gamma\": 0.99,\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 4,\n",
    "        \"num_envs_per_worker\": 1,\n",
    "        \"create_env_on_driver\": True,\n",
    "        \"lambda\": 0.95,\n",
    "        \"kl_coeff\": 0.5,\n",
    "        \"clip_rewards\": True,\n",
    "        \"clip_param\": 0.1,\n",
    "        \"vf_clip_param\": 10.0,\n",
    "        \"entropy_coeff\": 0.01,\n",
    "        \"rollout_fragment_length\": 100,\n",
    "        \"sgd_minibatch_size\": 500,\n",
    "        \"train_batch_size\": 2000,\n",
    "        \"num_sgd_iter\": 10,\n",
    "        \"recreate_failed_workers\":True,\n",
    "        \"ignore_worker_failures\":True,        \n",
    "        \"batch_mode\": \"truncate_episodes\",\n",
    "        \"observation_filter\": \"NoFilter\",\n",
    "        \"model\": {\n",
    "            \"vf_share_layers\": True,\n",
    "            \"custom_model\": \"cfc\",\n",
    "            \"max_seq_len\": 20,\n",
    "            \"custom_model_config\": {\n",
    "                \"cell_size\": 64,\n",
    "            },\n",
    "        },\n",
    "        \"framework\": \"tf2\",\n",
    "    }\n",
    "\n",
    "    algo = PPO(config=config)\n",
    "\n",
    "    os.makedirs(f\"rl_ckpt/{args.env}\", exist_ok=True)\n",
    "    if args.cont != \"\":\n",
    "        algo.load_checkpoint(f\"rl_ckpt/{args.env}/{args.cont}\")\n",
    "\n",
    "    if args.render == 2222:\n",
    "        run_closed_loop(\n",
    "            algo,\n",
    "            config,\n",
    "        )\n",
    "    else:\n",
    "        print(\"Training STarted\")\n",
    "        start_time = time.time()\n",
    "        last_eval = 0\n",
    "        while True:\n",
    "            info = algo.train()\n",
    "            if time.time() - last_eval > 60 * 2:  # every 5 minutes print some stats\n",
    "                print(f\"Ran {(time.time()-start_time)/60/60:0.1f} hours\")\n",
    "                print(\n",
    "                    f\"    sampled {info['info']['num_env_steps_sampled']/1000:0.0f}k steps\"\n",
    "                )\n",
    "                print(f\"    policy reward: {info['episode_reward_mean']:0.1f}\")\n",
    "                last_eval = time.time()\n",
    "                ckpt = algo.save_checkpoint(f\"rl_ckpt/{args.env}\")\n",
    "                print(f\"    saved checkpoint '{ckpt}'\")\n",
    "\n",
    "            elapsed = (time.time() - start_time) / 60  # in minutes\n",
    "            if elapsed > args.hours * 60:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5443168b-5d70-463a-98b4-3467ecb201c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"Common pre-checks for all RLlib experiments.\"\"\"\n",
      "from copy import copy\n",
      "import inspect\n",
      "import logging\n",
      "import gym\n",
      "import numpy as np\n",
      "import traceback\n",
      "import tree  # pip install dm_tree\n",
      "from typing import TYPE_CHECKING, Set\n",
      "\n",
      "from ray.actor import ActorHandle\n",
      "from ray.rllib.utils.annotations import DeveloperAPI\n",
      "from ray.rllib.utils.error import UnsupportedSpaceException\n",
      "from ray.rllib.utils.spaces.space_utils import (\n",
      "    convert_element_to_space_type,\n",
      "    get_base_struct_from_space,\n",
      ")\n",
      "from ray.rllib.utils.typing import EnvType\n",
      "from ray.util import log_once\n",
      "\n",
      "if TYPE_CHECKING:\n",
      "    from ray.rllib.env import BaseEnv, MultiAgentEnv\n",
      "\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "\n",
      "@DeveloperAPI\n",
      "def check_env(env: EnvType) -> None:\n",
      "    \"\"\"Run pre-checks on env that uncover common errors in environments.\n",
      "\n",
      "    Args:\n",
      "        env: Environment to be checked.\n",
      "\n",
      "    Raises:\n",
      "        ValueError: If env is not an instance of SUPPORTED_ENVIRONMENT_TYPES.\n",
      "        ValueError: See check_gym_env docstring for details.\n",
      "    \"\"\"\n",
      "    from ray.rllib.env import (\n",
      "        BaseEnv,\n",
      "        MultiAgentEnv,\n",
      "        RemoteBaseEnv,\n",
      "        VectorEnv,\n",
      "        ExternalMultiAgentEnv,\n",
      "        ExternalEnv,\n",
      "    )\n",
      "\n",
      "    if hasattr(env, \"_skip_env_checking\") and env._skip_env_checking:\n",
      "        # This is a work around for some environments that we already have in RLlb\n",
      "        # that we want to skip checking for now until we have the time to fix them.\n",
      "        if log_once(\"skip_env_checking\"):\n",
      "            logger.warning(\"Skipping env checking for this experiment\")\n",
      "        return\n",
      "\n",
      "    try:\n",
      "        if not isinstance(\n",
      "            env,\n",
      "            (\n",
      "                BaseEnv,\n",
      "                gym.Env,\n",
      "                MultiAgentEnv,\n",
      "                RemoteBaseEnv,\n",
      "                VectorEnv,\n",
      "                ExternalMultiAgentEnv,\n",
      "                ExternalEnv,\n",
      "                ActorHandle,\n",
      "            ),\n",
      "        ):\n",
      "            raise ValueError(\n",
      "                \"Env must be of one of the following supported types: BaseEnv, \"\n",
      "                \"gym.Env, \"\n",
      "                \"MultiAgentEnv, VectorEnv, RemoteBaseEnv, ExternalMultiAgentEnv, \"\n",
      "                f\"ExternalEnv, but instead is of type {type(env)}.\"\n",
      "            )\n",
      "        if isinstance(env, MultiAgentEnv):\n",
      "            check_multiagent_environments(env)\n",
      "        elif isinstance(env, gym.Env):\n",
      "            check_gym_environments(env)\n",
      "        elif isinstance(env, BaseEnv):\n",
      "            check_base_env(env)\n",
      "        else:\n",
      "            logger.warning(\n",
      "                \"Env checking isn't implemented for VectorEnvs, RemoteBaseEnvs, \"\n",
      "                \"ExternalMultiAgentEnv, ExternalEnvs or environments that are \"\n",
      "                \"Ray actors.\"\n",
      "            )\n",
      "    except Exception:\n",
      "        actual_error = traceback.format_exc()\n",
      "        raise ValueError(\n",
      "            f\"{actual_error}\\n\"\n",
      "            \"The above error has been found in your environment! \"\n",
      "            \"We've added a module for checking your custom environments. It \"\n",
      "            \"may cause your experiment to fail if your environment is not set up \"\n",
      "            \"correctly. You can disable this behavior by setting \"\n",
      "            \"`disable_env_checking=True` in your environment config \"\n",
      "            \"dictionary. You can run the environment checking module \"\n",
      "            \"standalone by calling ray.rllib.utils.check_env([env]).\"\n",
      "        )\n",
      "\n",
      "\n",
      "@DeveloperAPI\n",
      "def check_gym_environments(env: gym.Env) -> None:\n",
      "    \"\"\"Checking for common errors in gym environments.\n",
      "\n",
      "    Args:\n",
      "        env: Environment to be checked.\n",
      "\n",
      "    Warning:\n",
      "        If env has no attribute spec with a sub attribute,\n",
      "            max_episode_steps.\n",
      "\n",
      "    Raises:\n",
      "        AttributeError: If env has no observation space.\n",
      "        AttributeError: If env has no action space.\n",
      "        ValueError: Observation space must be a gym.spaces.Space.\n",
      "        ValueError: Action space must be a gym.spaces.Space.\n",
      "        ValueError: Observation sampled from observation space must be\n",
      "            contained in the observation space.\n",
      "        ValueError: Action sampled from action space must be\n",
      "            contained in the observation space.\n",
      "        ValueError: If env cannot be resetted.\n",
      "        ValueError: If an observation collected from a call to env.reset().\n",
      "            is not contained in the observation_space.\n",
      "        ValueError: If env cannot be stepped via a call to env.step().\n",
      "        ValueError: If the observation collected from env.step() is not\n",
      "            contained in the observation_space.\n",
      "        AssertionError: If env.step() returns a reward that is not an\n",
      "            int or float.\n",
      "        AssertionError: IF env.step() returns a done that is not a bool.\n",
      "        AssertionError: If env.step() returns an env_info that is not a dict.\n",
      "    \"\"\"\n",
      "\n",
      "    # check that env has observation and action spaces\n",
      "    if not hasattr(env, \"observation_space\"):\n",
      "        raise AttributeError(\"Env must have observation_space.\")\n",
      "    if not hasattr(env, \"action_space\"):\n",
      "        raise AttributeError(\"Env must have action_space.\")\n",
      "\n",
      "    # check that observation and action spaces are gym.spaces\n",
      "    if not isinstance(env.observation_space, gym.spaces.Space):\n",
      "        raise ValueError(\"Observation space must be a gym.space\")\n",
      "    if not isinstance(env.action_space, gym.spaces.Space):\n",
      "        raise ValueError(\"Action space must be a gym.space\")\n",
      "\n",
      "    # Raise a warning if there isn't a max_episode_steps attribute.\n",
      "    if not hasattr(env, \"spec\") or not hasattr(env.spec, \"max_episode_steps\"):\n",
      "        if log_once(\"max_episode_steps\"):\n",
      "            logger.warning(\n",
      "                \"Your env doesn't have a .spec.max_episode_steps \"\n",
      "                \"attribute. This is fine if you have set 'horizon' \"\n",
      "                \"in your config dictionary, or `soft_horizon`. \"\n",
      "                \"However, if you haven't, 'horizon' will default \"\n",
      "                \"to infinity, and your environment will not be \"\n",
      "                \"reset.\"\n",
      "            )\n",
      "    # Raise warning if using new reset api introduces in gym 0.24\n",
      "    reset_signature = inspect.signature(env.unwrapped.reset).parameters.keys()\n",
      "    if any(k in reset_signature for k in [\"seed\", \"return_info\"]):\n",
      "        if log_once(\"reset_signature\"):\n",
      "            logger.warning(\n",
      "                \"Your env reset() method appears to take 'seed' or 'return_info'\"\n",
      "                \" arguments. Note that these are not yet supported in RLlib.\"\n",
      "                \" Seeding will take place using 'env.seed()' and the info dict\"\n",
      "                \" will not be returned from reset.\"\n",
      "            )\n",
      "\n",
      "    # check if sampled actions and observations are contained within their\n",
      "    # respective action and observation spaces.\n",
      "\n",
      "    sampled_action = env.action_space.sample()\n",
      "    sampled_observation = env.observation_space.sample()\n",
      "    # Check if observation generated from stepping the environment is\n",
      "    # contained within the observation space.\n",
      "    reset_obs = env.reset()\n",
      "    if not env.observation_space.contains(reset_obs):\n",
      "        temp_sampled_reset_obs = convert_element_to_space_type(\n",
      "            reset_obs, sampled_observation\n",
      "        )\n",
      "        if not env.observation_space.contains(temp_sampled_reset_obs):\n",
      "            # Find offending subspace in case we have a complex observation space.\n",
      "            key, space, space_type, value, value_type = _find_offending_sub_space(\n",
      "                env.observation_space, temp_sampled_reset_obs\n",
      "            )\n",
      "            raise ValueError(\n",
      "                \"The observation collected from env.reset() was not \"\n",
      "                \"contained within your env's observation space. It is possible \"\n",
      "                \"that there was a type mismatch, or that one of the \"\n",
      "                \"sub-observations was out of bounds:\\n {}(sub-)obs: {} ({})\"\n",
      "                \"\\n (sub-)observation space: {} ({})\".format(\n",
      "                    (\"path: '\" + key + \"'\\n \") if key else \"\",\n",
      "                    value,\n",
      "                    value_type,\n",
      "                    space,\n",
      "                    space_type,\n",
      "                )\n",
      "            )\n",
      "    # Check if env.step can run, and generates observations rewards, done\n",
      "    # signals and infos that are within their respective spaces and are of\n",
      "    # the correct dtypes.\n",
      "    next_obs, reward, done, info = env.step(sampled_action)\n",
      "    if not env.observation_space.contains(next_obs):\n",
      "        temp_sampled_next_obs = convert_element_to_space_type(\n",
      "            next_obs, sampled_observation\n",
      "        )\n",
      "        if not env.observation_space.contains(temp_sampled_next_obs):\n",
      "            # Find offending subspace in case we have a complex observation space.\n",
      "            key, space, space_type, value, value_type = _find_offending_sub_space(\n",
      "                env.observation_space, temp_sampled_next_obs\n",
      "            )\n",
      "            error = (\n",
      "                \"The observation collected from env.step(sampled_action) was not \"\n",
      "                \"contained within your env's observation space. It is possible \"\n",
      "                \"that there was a type mismatch, or that one of the \"\n",
      "                \"sub-observations was out of bounds: \\n\\n {}(sub-)obs: {} ({})\"\n",
      "                \"\\n (sub-)observation space: {} ({})\".format(\n",
      "                    (\"path='\" + key + \"'\\n \") if key else \"\",\n",
      "                    value,\n",
      "                    value_type,\n",
      "                    space,\n",
      "                    space_type,\n",
      "                )\n",
      "            )\n",
      "            raise ValueError(error)\n",
      "    _check_done(done)\n",
      "    _check_reward(reward)\n",
      "    _check_info(info)\n",
      "\n",
      "\n",
      "@DeveloperAPI\n",
      "def check_multiagent_environments(env: \"MultiAgentEnv\") -> None:\n",
      "    \"\"\"Checking for common errors in RLlib MultiAgentEnvs.\n",
      "\n",
      "    Args:\n",
      "        env: The env to be checked.\n",
      "\n",
      "    \"\"\"\n",
      "    from ray.rllib.env import MultiAgentEnv\n",
      "\n",
      "    if not isinstance(env, MultiAgentEnv):\n",
      "        raise ValueError(\"The passed env is not a MultiAgentEnv.\")\n",
      "    elif not (\n",
      "        hasattr(env, \"observation_space\")\n",
      "        and hasattr(env, \"action_space\")\n",
      "        and hasattr(env, \"_agent_ids\")\n",
      "        and hasattr(env, \"_spaces_in_preferred_format\")\n",
      "    ):\n",
      "        if log_once(\"ma_env_super_ctor_called\"):\n",
      "            logger.warning(\n",
      "                f\"Your MultiAgentEnv {env} does not have some or all of the needed \"\n",
      "                \"base-class attributes! Make sure you call `super().__init__` from \"\n",
      "                \"within your MutiAgentEnv's constructor. \"\n",
      "                \"This will raise an error in the future.\"\n",
      "            )\n",
      "        return\n",
      "\n",
      "    reset_obs = env.reset()\n",
      "    sampled_obs = env.observation_space_sample()\n",
      "    _check_if_element_multi_agent_dict(env, reset_obs, \"reset()\")\n",
      "    _check_if_element_multi_agent_dict(\n",
      "        env, sampled_obs, \"env.observation_space_sample()\"\n",
      "    )\n",
      "\n",
      "    try:\n",
      "        env.observation_space_contains(reset_obs)\n",
      "    except Exception as e:\n",
      "        raise ValueError(\n",
      "            \"Your observation_space_contains function has some error \"\n",
      "        ) from e\n",
      "\n",
      "    if not env.observation_space_contains(reset_obs):\n",
      "        error = (\n",
      "            _not_contained_error(\"env.reset\", \"observation\")\n",
      "            + f\"\\n\\n reset_obs: {reset_obs}\\n\\n env.observation_space_sample():\"\n",
      "            f\" {sampled_obs}\\n\\n \"\n",
      "        )\n",
      "        raise ValueError(error)\n",
      "\n",
      "    if not env.observation_space_contains(sampled_obs):\n",
      "        error = (\n",
      "            _not_contained_error(\"observation_space_sample\", \"observation\")\n",
      "            + f\"\\n\\n env.observation_space_sample():\"\n",
      "            f\" {sampled_obs}\\n\\n \"\n",
      "        )\n",
      "        raise ValueError(error)\n",
      "\n",
      "    sampled_action = env.action_space_sample(reset_obs.keys())\n",
      "    _check_if_element_multi_agent_dict(env, sampled_action, \"action_space_sample\")\n",
      "    try:\n",
      "        env.action_space_contains(sampled_action)\n",
      "    except Exception as e:\n",
      "        raise ValueError(\"Your action_space_contains function has some error \") from e\n",
      "\n",
      "    if not env.action_space_contains(sampled_action):\n",
      "        error = (\n",
      "            _not_contained_error(\"action_space_sample\", \"action\")\n",
      "            + f\"\\n\\n sampled_action {sampled_action}\\n\\n\"\n",
      "        )\n",
      "        raise ValueError(error)\n",
      "\n",
      "    next_obs, reward, done, info = env.step(sampled_action)\n",
      "    _check_if_element_multi_agent_dict(env, next_obs, \"step, next_obs\")\n",
      "    _check_if_element_multi_agent_dict(env, reward, \"step, reward\")\n",
      "    _check_if_element_multi_agent_dict(env, done, \"step, done\")\n",
      "    _check_if_element_multi_agent_dict(env, info, \"step, info\")\n",
      "    _check_reward(\n",
      "        {\"dummy_env_id\": reward}, base_env=True, agent_ids=env.get_agent_ids()\n",
      "    )\n",
      "    _check_done({\"dummy_env_id\": done}, base_env=True, agent_ids=env.get_agent_ids())\n",
      "    _check_info({\"dummy_env_id\": info}, base_env=True, agent_ids=env.get_agent_ids())\n",
      "    if not env.observation_space_contains(next_obs):\n",
      "        error = (\n",
      "            _not_contained_error(\"env.step(sampled_action)\", \"observation\")\n",
      "            + f\":\\n\\n next_obs: {next_obs} \\n\\n sampled_obs: {sampled_obs}\"\n",
      "        )\n",
      "        raise ValueError(error)\n",
      "\n",
      "\n",
      "@DeveloperAPI\n",
      "def check_base_env(env: \"BaseEnv\") -> None:\n",
      "    \"\"\"Checking for common errors in RLlib BaseEnvs.\n",
      "\n",
      "    Args:\n",
      "        env: The env to be checked.\n",
      "\n",
      "    \"\"\"\n",
      "    from ray.rllib.env import BaseEnv\n",
      "\n",
      "    if not isinstance(env, BaseEnv):\n",
      "        raise ValueError(\"The passed env is not a BaseEnv.\")\n",
      "\n",
      "    reset_obs = env.try_reset()\n",
      "    sampled_obs = env.observation_space_sample()\n",
      "    _check_if_multi_env_dict(env, reset_obs, \"try_reset\")\n",
      "    _check_if_multi_env_dict(env, sampled_obs, \"observation_space_sample()\")\n",
      "\n",
      "    try:\n",
      "        env.observation_space_contains(reset_obs)\n",
      "    except Exception as e:\n",
      "        raise ValueError(\n",
      "            \"Your observation_space_contains function has some error \"\n",
      "        ) from e\n",
      "\n",
      "    if not env.observation_space_contains(reset_obs):\n",
      "        error = (\n",
      "            _not_contained_error(\"try_reset\", \"observation\")\n",
      "            + f\": \\n\\n reset_obs: {reset_obs}\\n\\n \"\n",
      "            f\"env.observation_space_sample(): {sampled_obs}\\n\\n \"\n",
      "        )\n",
      "        raise ValueError(error)\n",
      "\n",
      "    if not env.observation_space_contains(sampled_obs):\n",
      "        error = (\n",
      "            _not_contained_error(\"observation_space_sample\", \"observation\")\n",
      "            + f\": \\n\\n sampled_obs: {sampled_obs}\\n\\n \"\n",
      "        )\n",
      "        raise ValueError(error)\n",
      "\n",
      "    sampled_action = env.action_space_sample()\n",
      "    try:\n",
      "        env.action_space_contains(sampled_action)\n",
      "    except Exception as e:\n",
      "        raise ValueError(\"Your action_space_contains function has some error \") from e\n",
      "    if not env.action_space_contains(sampled_action):\n",
      "        error = (\n",
      "            _not_contained_error(\"action_space_sample\", \"action\")\n",
      "            + f\": \\n\\n sampled_action {sampled_action}\\n\\n\"\n",
      "        )\n",
      "        raise ValueError(error)\n",
      "    _check_if_multi_env_dict(env, sampled_action, \"action_space_sample()\")\n",
      "\n",
      "    env.send_actions(sampled_action)\n",
      "\n",
      "    next_obs, reward, done, info, _ = env.poll()\n",
      "    _check_if_multi_env_dict(env, next_obs, \"step, next_obs\")\n",
      "    _check_if_multi_env_dict(env, reward, \"step, reward\")\n",
      "    _check_if_multi_env_dict(env, done, \"step, done\")\n",
      "    _check_if_multi_env_dict(env, info, \"step, info\")\n",
      "\n",
      "    if not env.observation_space_contains(next_obs):\n",
      "        error = (\n",
      "            _not_contained_error(\"poll\", \"observation\")\n",
      "            + f\": \\n\\n reset_obs: {reset_obs}\\n\\n env.step():{next_obs}\\n\\n\"\n",
      "        )\n",
      "        raise ValueError(error)\n",
      "\n",
      "    _check_reward(reward, base_env=True, agent_ids=env.get_agent_ids())\n",
      "    _check_done(done, base_env=True, agent_ids=env.get_agent_ids())\n",
      "    _check_info(info, base_env=True, agent_ids=env.get_agent_ids())\n",
      "\n",
      "\n",
      "def _check_reward(reward, base_env=False, agent_ids=None):\n",
      "    if base_env:\n",
      "        for _, multi_agent_dict in reward.items():\n",
      "            for agent_id, rew in multi_agent_dict.items():\n",
      "                if not (\n",
      "                    np.isreal(rew)\n",
      "                    and not isinstance(rew, bool)\n",
      "                    and (\n",
      "                        np.isscalar(rew)\n",
      "                        or (isinstance(rew, np.ndarray) and rew.shape == ())\n",
      "                    )\n",
      "                ):\n",
      "                    error = (\n",
      "                        \"Your step function must return rewards that are\"\n",
      "                        f\" integer or float. reward: {rew}. Instead it was a \"\n",
      "                        f\"{type(rew)}\"\n",
      "                    )\n",
      "                    raise ValueError(error)\n",
      "                if not (agent_id in agent_ids or agent_id == \"__all__\"):\n",
      "                    error = (\n",
      "                        f\"Your reward dictionary must have agent ids that belong to \"\n",
      "                        f\"the environment. Agent_ids recieved from \"\n",
      "                        f\"env.get_agent_ids() are: {agent_ids}\"\n",
      "                    )\n",
      "                    raise ValueError(error)\n",
      "    elif not (\n",
      "        np.isreal(reward)\n",
      "        and not isinstance(reward, bool)\n",
      "        and (\n",
      "            np.isscalar(reward)\n",
      "            or (isinstance(reward, np.ndarray) and reward.shape == ())\n",
      "        )\n",
      "    ):\n",
      "        error = (\n",
      "            \"Your step function must return a reward that is integer or float. \"\n",
      "            \"Instead it was a {}\".format(type(reward))\n",
      "        )\n",
      "        raise ValueError(error)\n",
      "\n",
      "\n",
      "def _check_done(done, base_env=False, agent_ids=None):\n",
      "    if base_env:\n",
      "        for _, multi_agent_dict in done.items():\n",
      "            for agent_id, done_ in multi_agent_dict.items():\n",
      "                if not isinstance(done_, (bool, np.bool, np.bool_)):\n",
      "                    raise ValueError(\n",
      "                        \"Your step function must return dones that are boolean. But \"\n",
      "                        f\"instead was a {type(done)}\"\n",
      "                    )\n",
      "                if not (agent_id in agent_ids or agent_id == \"__all__\"):\n",
      "                    error = (\n",
      "                        f\"Your dones dictionary must have agent ids that belong to \"\n",
      "                        f\"the environment. Agent_ids recieved from \"\n",
      "                        f\"env.get_agent_ids() are: {agent_ids}\"\n",
      "                    )\n",
      "                    raise ValueError(error)\n",
      "    elif not isinstance(done, (bool, np.bool_)):\n",
      "        error = (\n",
      "            \"Your step function must return a done that is a boolean. But instead \"\n",
      "            f\"was a {type(done)}\"\n",
      "        )\n",
      "        raise ValueError(error)\n",
      "\n",
      "\n",
      "def _check_info(info, base_env=False, agent_ids=None):\n",
      "    if base_env:\n",
      "        for _, multi_agent_dict in info.items():\n",
      "            for agent_id, inf in multi_agent_dict.items():\n",
      "                if not isinstance(inf, dict):\n",
      "                    raise ValueError(\n",
      "                        \"Your step function must return infos that are a dict. \"\n",
      "                        f\"instead was a {type(inf)}: element: {inf}\"\n",
      "                    )\n",
      "                if not (agent_id in agent_ids or agent_id == \"__all__\"):\n",
      "                    error = (\n",
      "                        f\"Your dones dictionary must have agent ids that belong to \"\n",
      "                        f\"the environment. Agent_ids recieved from \"\n",
      "                        f\"env.get_agent_ids() are: {agent_ids}\"\n",
      "                    )\n",
      "                    raise ValueError(error)\n",
      "    elif not isinstance(info, dict):\n",
      "        error = (\n",
      "            \"Your step function must return a info that \"\n",
      "            f\"is a dict. element type: {type(info)}. element: {info}\"\n",
      "        )\n",
      "        raise ValueError(error)\n",
      "\n",
      "\n",
      "def _not_contained_error(func_name, _type):\n",
      "    _error = (\n",
      "        f\"The {_type} collected from {func_name} was not contained within\"\n",
      "        f\" your env's {_type} space. Its possible that there was a type\"\n",
      "        f\"mismatch (for example {_type}s of np.float32 and a space of\"\n",
      "        f\"np.float64 {_type}s), or that one of the sub-{_type}s was\"\n",
      "        f\"out of bounds\"\n",
      "    )\n",
      "    return _error\n",
      "\n",
      "\n",
      "def _check_if_multi_env_dict(env, element, function_string):\n",
      "    if not isinstance(element, dict):\n",
      "        raise ValueError(\n",
      "            f\"The element returned by {function_string} is not a \"\n",
      "            f\"MultiEnvDict. Instead, it is of type: {type(element)}\"\n",
      "        )\n",
      "    env_ids = env.get_sub_environments(as_dict=True).keys()\n",
      "    if not all(k in env_ids for k in element):\n",
      "        raise ValueError(\n",
      "            f\"The element returned by {function_string} \"\n",
      "            f\"has dict keys that don't correspond to \"\n",
      "            f\"environment ids for this env \"\n",
      "            f\"{list(env_ids)}\"\n",
      "        )\n",
      "    for _, multi_agent_dict in element.items():\n",
      "        _check_if_element_multi_agent_dict(\n",
      "            env, multi_agent_dict, function_string, base_env=True\n",
      "        )\n",
      "\n",
      "\n",
      "def _check_if_element_multi_agent_dict(env, element, function_string, base_env=False):\n",
      "    if not isinstance(element, dict):\n",
      "        if base_env:\n",
      "            error = (\n",
      "                f\"The element returned by {function_string} contains values \"\n",
      "                f\"that are not MultiAgentDicts. Instead, they are of \"\n",
      "                f\"type: {type(element)}\"\n",
      "            )\n",
      "        else:\n",
      "            error = (\n",
      "                f\"The element returned by {function_string} is not a \"\n",
      "                f\"MultiAgentDict. Instead, it is of type: \"\n",
      "                f\" {type(element)}\"\n",
      "            )\n",
      "        raise ValueError(error)\n",
      "    agent_ids: Set = copy(env.get_agent_ids())\n",
      "    agent_ids.add(\"__all__\")\n",
      "\n",
      "    if not all(k in agent_ids for k in element):\n",
      "        if base_env:\n",
      "            error = (\n",
      "                f\"The element returned by {function_string} has agent_ids\"\n",
      "                f\" that are not the names of the agents in the env.\"\n",
      "                f\"agent_ids in this\\nMultiEnvDict:\"\n",
      "                f\" {list(element.keys())}\\nAgent_ids in this env:\"\n",
      "                f\"{list(env.get_agent_ids())}\"\n",
      "            )\n",
      "        else:\n",
      "            error = (\n",
      "                f\"The element returned by {function_string} has agent_ids\"\n",
      "                f\" that are not the names of the agents in the env. \"\n",
      "                f\"\\nAgent_ids in this MultiAgentDict: \"\n",
      "                f\"{list(element.keys())}\\nAgent_ids in this env:\"\n",
      "                f\"{list(env.get_agent_ids())}. You likely need to add the private \"\n",
      "                f\"attribute `_agent_ids` to your env, which is a set containing the \"\n",
      "                f\"ids of agents supported by your env.\"\n",
      "            )\n",
      "        raise ValueError(error)\n",
      "\n",
      "\n",
      "def _find_offending_sub_space(space, value):\n",
      "    \"\"\"Returns error, value, and space when offending `space.contains(value)` fails.\n",
      "\n",
      "    Returns only the offending sub-value/sub-space in case `space` is a complex Tuple\n",
      "    or Dict space.\n",
      "\n",
      "    Args:\n",
      "        space: The gym.Space to check.\n",
      "        value: The actual (numpy) value to check for matching `space`.\n",
      "\n",
      "    Returns:\n",
      "        Tuple consisting of 1) key-sequence of the offending sub-space or the empty\n",
      "        string if `space` is not complex (Tuple or Dict), 2) the offending sub-space,\n",
      "        3) the offending sub-space's dtype, 4) the offending sub-value, 5) the offending\n",
      "        sub-value's dtype.\n",
      "\n",
      "    Examples:\n",
      "         >>> path, space, space_dtype, value, value_dtype = _find_offending_sub_space(\n",
      "         ...     gym.spaces.Dict({\n",
      "         ...    -2.0, 1.5, (2, ), np.int8), np.array([-1.5, 3.0])\n",
      "         ... )\n",
      "         >>> print(path)\n",
      "         ...\n",
      "    \"\"\"\n",
      "    if not isinstance(space, (gym.spaces.Dict, gym.spaces.Tuple)):\n",
      "        return None, space, space.dtype, value, _get_type(value)\n",
      "\n",
      "    structured_space = get_base_struct_from_space(space)\n",
      "\n",
      "    def map_fn(p, s, v):\n",
      "        if not s.contains(v):\n",
      "            raise UnsupportedSpaceException((p, s, v))\n",
      "\n",
      "    try:\n",
      "        tree.map_structure_with_path(map_fn, structured_space, value)\n",
      "    except UnsupportedSpaceException as e:\n",
      "        space, value = e.args[0][1], e.args[0][2]\n",
      "        return \"->\".join(e.args[0][0]), space, space.dtype, value, _get_type(value)\n",
      "\n",
      "    # This is actually an error.\n",
      "    return None, None, None, None, None\n",
      "\n",
      "\n",
      "def _get_type(var):\n",
      "    return var.dtype if hasattr(var, \"dtype\") else type(var)\n"
     ]
    }
   ],
   "source": [
    "!cat /home/silence/Projects/ForexLab/dataset/episode2/env10/lib/python3.10/site-packages/ray/rllib/utils/pre_checks/env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da5410-c2a8-4bf6-b39e-fbe4ac9871c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
